{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modern-asset",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-04-04T17:10:20.869941+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.5\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harold.patino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model  import LinearRegression as LinReg\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import cloudscraper\n",
    "\n",
    "import es_core_news_sm\n",
    "nltk.download('stopwords')\n",
    "nlp = es_core_news_sm.load()\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from parsel import Selector\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import scrapy\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# Funciones adicionales\n",
    "error_page = []\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "delta = timedelta(days=1)\n",
    "ayer = now - delta\n",
    "ayer = ayer.strftime('%d/%m/%Y')\n",
    "\n",
    "# Funcion para agregar fechas sin inicio de cero\n",
    "dt_string_adicional = dt_string[1:]\n",
    "ayer_adicional = ayer[1:]\n",
    "\n",
    "\n",
    "def time_date(time):\n",
    "    fecha = []\n",
    "    for item in time:\n",
    "        fecha.append(datetime.fromtimestamp(int(item)).strftime('%d/%m/%Y'))\n",
    "    return(fecha)\n",
    "\n",
    "def MesNumero(string):\n",
    "    m = {'enero': \"01\",'january': \"01\",'febrero': \"02\",'february': \"02\",'marzo': \"03\",'march': \"03\",'abril': \"04\",'april': \"04\",'mayo': \"05\",'may': \"05\",'junio': \"06\",'june': \"06\",'julio': \"07\",'july': \"07\",'agosto': \"08\",'august': \"08\",'septiembre': \"09\",'september': \"09\",'octubre': \"10\",'october': \"10\",'noviembre': \"11\",'november': \"11\",'diciembre': \"12\",'december': \"12\"}\n",
    "    mcorto = {'ene': \"01\",'jan': \"01\",'feb': \"02\",'mar': \"03\",'abr': \"04\",'apr': \"04\",'may': \"05\",'jun': \"06\",'jul': \"07\",'ago': \"08\",'aug': \"08\",'sep': \"09\",'oct': \"10\",'nov': \"11\",'dic': \"12\",'dec': \"12\"}\n",
    "\n",
    "    fecha = string.split(\"-\")\n",
    "    dia =  fecha[0]\n",
    "    mes =  fecha[1]\n",
    "    anio = fecha[2]\n",
    "\n",
    "    try:\n",
    "        if len(mes) > 3:\n",
    "            mes_out = str(m[mes.lower()])\n",
    "        else:\n",
    "            mes_out = str(mcorto[mes.lower()])\n",
    "        \n",
    "        return(dia + \"/\" +  mes_out + \"/\" + anio)\n",
    "    except:\n",
    "        raise ValueError('No es un mes')\n",
    "\n",
    "def limpieza(fuentes):\n",
    "    agregar_dato=[]\n",
    "    for item in fuentes:\n",
    "        agregar_dato.append(item.replace('\\xa0', ''))\n",
    "    return(agregar_dato)\n",
    "\n",
    "def limpieza_descripcion(variable_corregir):\n",
    "    valor_limpio = list(map(lambda d: d.replace('\\r\\n', ''), variable_corregir))\n",
    "    valor_limpio = [i.strip('\\n') for i in valor_limpio]\n",
    "    valor_limpio = [i.strip('\\r') for i in valor_limpio]\n",
    "    valor_limpio = limpieza(valor_limpio)\n",
    "    \n",
    "    variable_almacenamiento = []\n",
    "    for x in valor_limpio:\n",
    "        x = remove_tags(x)\n",
    "        variable_almacenamiento.append(x)\n",
    "    return (variable_almacenamiento)\n",
    "\n",
    "def remove_caracter(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\t ', '', text) \n",
    "    return text\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub(':.*?:', '', text)\n",
    "    text = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", text), 0, re.I)\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace(';', '')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def deletestopwords(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    stop_words_sp = STOP_WORDS # WITH SPACY\n",
    "    word_tokens_clean = [each for each in word_tokens if each.lower() not in stop_words_sp and len(each.lower()) > 2]\n",
    "    return word_tokens_clean\n",
    "\n",
    "def lemmatizer(text):  \n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc])\n",
    "\n",
    "def formatingText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub(':.*?:', '', text)\n",
    "    text = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", text), 0, re.I)\n",
    "    text = normalize( 'NFC', text)\n",
    "    text = re.sub('[^a-z ]', '', text)\n",
    "    return text\n",
    "\n",
    "def lemant(text1):\n",
    "    x=text1.lower()\n",
    "    stopwords= deletestopwords(x)\n",
    "    stopwords=str(stopwords)\n",
    "    clean = formatingText(stopwords)\n",
    "    final = lemmatizer(clean)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-hartford",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"C:/Users/harold.patino/Documents/Periodicos/DeVops/stopwords-es.json\") as fname:\n",
    "    stopwords_es = json.load(fname)\n",
    "vectorizar_news = TfidfVectorizer(strip_accents='unicode', stop_words=stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serious-disabled",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "\n",
    "news = pd.read_csv(\"C:/Users/harold.patino/Documents/lectura_archivos/Data/Noticias_Data_Entry.csv\" , sep=';')\n",
    "# news = news[['REVIEW','TIP_EVEN','CAT_EVENTO']]\n",
    "\n",
    "# http://rasbt.github.io/mlxtend/\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "# vectorizador =  TfidfVectorizer(strip_accents='unicode', stop_words=STOP_WORDS)\n",
    "# vectorizador.fit(news.REVIEW)\n",
    "\n",
    "# def model_type():\n",
    "    \n",
    "#     pipeline_logistica = make_pipeline (\n",
    "#     vectorizador,\n",
    "#     DenseTransformer(),\n",
    "#     LogisticRegression(C=5, verbose=1, random_state=0, penalty='l2')\n",
    "#     )\n",
    "#     modelo = pipeline_logistica.fit(X=news.REVIEW, y= news.TIP_EVEN)\n",
    "#     return modelo\n",
    "\n",
    "\n",
    "# def model_cat():\n",
    "   \n",
    "#     pipeline_logistica = make_pipeline (\n",
    "#     vectorizador,\n",
    "#     DenseTransformer(),\n",
    "#     LogisticRegression(C=5, verbose=1, random_state=0, penalty='l2')\n",
    "#     )\n",
    "#     modelo = pipeline_logistica.fit(X=news.REVIEW, y= news.CAT_EVENTO)\n",
    "#     return modelo\n",
    "\n",
    "# predCateg = model_cat()\n",
    "# predType = model_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_t = pickle.dump(predType, open('model_t.pkl', 'wb'))\n",
    "# modelo_c = pickle.dump(predCateg, open('model_c.pkl', 'wb'))\n",
    "# modelo_s = pickle.dump(predSubtype, open('model_s.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passing-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "predType = pickle.load(open('model_t.pkl', 'rb'))\n",
    "predCateg = pickle.load(open('model_c.pkl', 'rb'))\n",
    "# predSubtype = pickle.load(open('model_s.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-cincinnati",
   "metadata": {},
   "source": [
    "## Periodicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unlike-sitting",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\harold.patino\\.conda\\envs\\data\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afirmo', 'agrego', 'algun', 'anadio', 'aseguro', 'comento', 'considero', 'dejo', 'demas', 'estara', 'explico', 'expreso', 'habian', 'habra', 'indico', 'llego', 'manifesto', 'menciono', 'ningun', 'podra', 'podran', 'proximos', 'quedo', 'realizo', 'senalo', 'seran', 'seria', 'tendra', 'tendran', 'tenia', 'traves', 'ultima', 'ultimas', 'ultimos'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "c:\\users\\harold.patino\\.conda\\envs\\data\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afirmo', 'agrego', 'algun', 'anadio', 'aseguro', 'comento', 'considero', 'dejo', 'demas', 'estara', 'explico', 'expreso', 'habian', 'habra', 'indico', 'llego', 'manifesto', 'menciono', 'ningun', 'podra', 'podran', 'proximos', 'quedo', 'realizo', 'senalo', 'seran', 'seria', 'tendra', 'tendran', 'tenia', 'traves', 'ultima', 'ultimas', 'ultimos'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# El Nuevo Siglo\n",
    "\n",
    "def fuentes_siglo (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h3 a::text, span.field.field--name-title.field--type-string.field--label-hidden ::text\").extract()\n",
    "        titulo = [i.replace('\\xa0', ' ') for i in titulo]\n",
    "        categoria = sel.css(\"h1 ::text\").extract()[0]\n",
    "        descripcion = sel.css(\"div.body ::text\").extract()\n",
    "        descripcion = limpieza(descripcion)\n",
    "        descripcion = [i.split('\\n\\n')[0] for i in descripcion]\n",
    "        descripcion = [i.split(' \\n')[0] for i in descripcion]\n",
    "        descripcion = pd.DataFrame(descripcion)\n",
    "        descripcion = descripcion.drop(descripcion[descripcion[0]=='\\n     '].index)\n",
    "        descripcion = descripcion.drop(descripcion[descripcion[0]=='\\n       '].index)\n",
    "        descripcion = descripcion.values.tolist()\n",
    "        descripcion = [item for lista in descripcion for item in lista]\n",
    "        descripcion = [i.replace('\\n','') for i in descripcion]\n",
    "\n",
    "        url = sel.css(\"h3 a::attr(href), h2 a ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4] =='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)\n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.elnuevosiglo.com.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.elnuevosiglo.com.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.clearfix.text-formatted.field.field--name-body.field--type-text-with-summary.field--label-hidden.field__item p ::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.strip('\\n') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.strip('\\r') for i in descripcion_larga] \n",
    "                descripcion_larga = [i.strip('|') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo)== len(descripcion):   \n",
    "            noticias = {}\n",
    "            noticias['title'] =  titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  dt_string\n",
    "            noticias['medium'] = \"El Nuevo Siglo\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] =  titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  dt_string\n",
    "            noticias['medium'] = \"El Nuevo Siglo\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        df_siglo=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_siglo\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_nacion = fuentes_siglo('https://www.elnuevosiglo.com.co/seccion/nacion')\n",
    "fuente_politica = fuentes_siglo('https://www.elnuevosiglo.com.co/seccion/politica')\n",
    "fuente_economia = fuentes_siglo('https://www.elnuevosiglo.com.co/seccion/economia')\n",
    "\n",
    "if fuente_nacion is None and fuente_politica is None and fuente_economia is None:\n",
    "    df_siglo= pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_siglo = pd.concat([fuente_nacion, fuente_politica, fuente_economia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "clear-banks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# El Nuevo Dia\n",
    "\n",
    "def fuentes_dia (periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "        if r_noticias.status_code==200:\n",
    "            \n",
    "            titulo = sel.css(\"span.field.field--name-title.field--type-string.field--label-hidden ::text\").extract()\n",
    "            descripcion = sel.css(\"div.field.field--name-field-entradilla.field--type-string-long.field--label-hidden.field__item ::text\").extract()\n",
    "            descripcion = limpieza_descripcion(descripcion)\n",
    "            descripcion = pd.DataFrame(descripcion)\n",
    "            descripcion = descripcion.replace('', 'NaN')\n",
    "            descripcion = descripcion.drop(descripcion[descripcion[0]=='NaN'].index)\n",
    "            descripcion = descripcion.values.tolist()\n",
    "            descripcion = [item for lista in descripcion for item in lista]\n",
    "            categoria = periodico_url.split('/')[-1]\n",
    "            fecha = sel.css(\"time.datetime\").extract()\n",
    "            fecha = [i.split('T')[0] for i in fecha]\n",
    "            fecha = [i.split('\"')[-1] for i in fecha]\n",
    "\n",
    "            url = sel.css(\"h2.node__title.title a ::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.elnuevodia.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.elnuevodia.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.clearfix.text-formatted.field.field--name-body.field--type-text-with-summary.field--label-hidden.field__item p ::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            fecha_nuevo = []\n",
    "            for f in fecha:\n",
    "                f = f.split('-')\n",
    "                dia = f[2]\n",
    "                mes = f[1]\n",
    "                anio = f[0]\n",
    "                fecha_nuevo.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "\n",
    "            if len(titulo) == len(descripcion):\n",
    "                descripcion = descripcion\n",
    "            else:\n",
    "                descripcion = titulo\n",
    "\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0]\n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            if len(titulo) == len(descripcion):          \n",
    "                noticias = {}\n",
    "                noticias['title'] =  titulo\n",
    "                noticias['description'] = descripcion\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['pubDate'] =  fecha_nuevo\n",
    "                noticias['medium'] = \"El Nuevo Dia\"\n",
    "                noticias['category'] = categoria\n",
    "                noticias['link'] = variable_url\n",
    "                #--  Predict  \n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "            else:\n",
    "                noticias = {}\n",
    "                noticias['title'] =  titulo\n",
    "                noticias['description'] = titulo\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['pubDate'] =  fecha_nuevo\n",
    "                noticias['medium'] = \"El Nuevo Dia\"\n",
    "                noticias['category'] = categoria\n",
    "                noticias['link'] = variable_url\n",
    "                #--  Predict  \n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "\n",
    "\n",
    "            df_dia = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category', 'medium', 'processDate', 'predi_type', 'predi_categ'])\n",
    "            return df_dia\n",
    "        \n",
    "        else:\n",
    "            print(periodico_url) \n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_judicial = fuentes_dia('http://www.elnuevodia.com.co/nuevodia/judicial')\n",
    "fuente_tolima = fuentes_dia('http://www.elnuevodia.com.co/nuevodia/tolima')\n",
    "fuente_ibague = fuentes_dia('http://www.elnuevodia.com.co/nuevodia/ibague')\n",
    "fuente_regional = fuentes_dia('http://www.elnuevodia.com.co/nuevodia/archivo-columnista/regional')\n",
    "\n",
    "\n",
    "if fuente_judicial is None and fuente_tolima is None and fuente_ibague is None and fuente_regional is None:\n",
    "    df_nuevodia = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_nuevodia = pd.concat([fuente_judicial, fuente_tolima, fuente_ibague, fuente_regional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-classification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EL Pais (Cali)\n",
    "def fuentes_pais (periodico_url):\n",
    "    try:\n",
    "        r=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r.content, features =\"xml\")\n",
    "        sel=Selector(r.text)\n",
    "        items = soup.findAll('item')\n",
    "\n",
    "        news_items = []\n",
    "        for item in items:\n",
    "            news_item = {}\n",
    "            news_item['title'] = item.title.text\n",
    "            news_item['description'] = news_item['title']\n",
    "            news_item['pubDate'] = item.pubDate.text \n",
    "            news_item['pubDate'] = news_item['pubDate'].split(',')[-1]\n",
    "            news_item['pubDate'] = news_item['pubDate'].split(' ')\n",
    "            dia = news_item['pubDate'][1]\n",
    "            mes = news_item['pubDate'][2]\n",
    "            anio = news_item['pubDate'][3]\n",
    "            news_item['pubDate'] = (dia + \"-\" +  mes + \"-\" + anio)\n",
    "            news_item['pubDate'] = MesNumero(news_item['pubDate'])\n",
    "            news_item['category']= item.category.text\n",
    "            variable_des = ()\n",
    "            news_item['link'] = item.link.text\n",
    "            r = requests.get(news_item['link'], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r.text)\n",
    "            if r.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.article-content p ::text, div.article-content strong ::text\").extract()            \n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des = ''.join(valor)\n",
    "                else:\n",
    "                    variable_des = ''.join(descripcion_larga)\n",
    "            else:\n",
    "                variable_des = ''.join('Error, pagina no encontrada')\n",
    "\n",
    "            news_item['description_long'] = variable_des\n",
    "            news_item['medium']=\"El Pais\"  \n",
    "            #--  Predict\n",
    "            vect_text1 = lemant(news_item['title'])\n",
    "            news_item['predi_categ'] = predCateg.predict([vect_text1])[0]\n",
    "            news_item['predi_type'] = predType.predict([vect_text1])[0] \n",
    "            news_items.append(news_item)\n",
    "\n",
    "        df_noticias_elpais = pd.DataFrame(news_items,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_noticias_elpais\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_colombia = fuentes_pais('https://www.elpais.com.co/rss/colombia')\n",
    "fuente_judicial = fuentes_pais('https://www.elpais.com.co/rss/judicial')\n",
    "fuente_cali = fuentes_pais('https://www.elpais.com.co/rss/cali')\n",
    "fuente_valle = fuentes_pais('https://www.elpais.com.co/rss/valle')\n",
    "\n",
    "if fuente_colombia is None and fuente_judicial is None and fuente_cali is None and fuente_valle is None:\n",
    "    df_elpais = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_elpais = pd.concat([fuente_colombia, fuente_judicial, fuente_cali, fuente_valle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La Nación y El Lider, dos periodicos misma fuente URL (Huila)\n",
    "def fuentes_la_nacion(periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r.content, features =\"xml\")\n",
    "        sel=Selector(r.text)\n",
    "        items = soup.findAll('item')\n",
    "\n",
    "        news_items = []\n",
    "        for item in items:\n",
    "            news_item = {}\n",
    "            news_item['title'] = item.title.text\n",
    "            news_item['description']= item.description.text\n",
    "            news_item['pubDate'] = item.pubDate.text\n",
    "            news_item['pubDate'] = news_item['pubDate'].split(',')[-1]\n",
    "            news_item['pubDate'] = news_item['pubDate'].split(' ')\n",
    "            dia = news_item['pubDate'][1]\n",
    "            mes = news_item['pubDate'][2]\n",
    "            anio = news_item['pubDate'][3]\n",
    "            news_item['pubDate'] = (dia + \"-\" +  mes + \"-\" + anio)\n",
    "            news_item['pubDate'] = MesNumero(news_item['pubDate'])\n",
    "            news_item['category']= item.category.text\n",
    "            variable_des = ()\n",
    "            news_item['link'] = item.link.text\n",
    "            r = requests.get(news_item['link'], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r.text)\n",
    "            if r.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.penci-entry-content.entry-content p ::text, div.penci-entry-content.entry-content strong ::text\").extract()           \n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des = ''.join(valor)\n",
    "                else:\n",
    "                    variable_des = ''.join(descripcion_larga)\n",
    "            else:\n",
    "                variable_des = ''.join('Error, pagina no encontrada')\n",
    "\n",
    "            news_item['description_long'] = variable_des\n",
    "            news_item['medium']=\"La Nacion\" \n",
    "            #--  Predict\n",
    "            vect_text1 = lemant(news_item['description'])\n",
    "            news_item['predi_categ'] = predCateg.predict([vect_text1])[0]\n",
    "            news_item['predi_type'] = predType.predict([vect_text1])[0] \n",
    "            news_items.append(news_item) \n",
    "            \n",
    "        la_nacion = pd.DataFrame(news_items,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return la_nacion\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "nacion_huila = fuentes_la_nacion('https://www.lanacion.com.co/category/regional-huila/feed/')\n",
    "nacion_judicial = fuentes_la_nacion('https://www.lanacion.com.co/category/judicial/feed/')\n",
    "\n",
    "if nacion_huila is None and nacion_judicial is None:\n",
    "    df_la_nacion = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_la_nacion = pd.concat([nacion_huila, nacion_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diario del Norte - Nombre Periodico: La Guajira Hoy, Ciudad: Guajira\n",
    "def fuentes_guajira_hoy(periodico_url):\n",
    "    try:\n",
    "        r=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r.content, features =\"xml\")\n",
    "        sel=Selector(r.text)\n",
    "        items = soup.findAll('item')\n",
    "\n",
    "\n",
    "        news_items = []\n",
    "        for item in items:\n",
    "            news_item = {}\n",
    "            news_item['title'] = item.title.text\n",
    "            news_item['description']= item.description.text\n",
    "            news_item['description']= remove_tags(news_item['description']).lstrip('<p>')\n",
    "            news_item['description'] = lemant(news_item['description'])\n",
    "            news_item['pubDate'] = item.pubDate.text \n",
    "            news_item['pubDate'] = news_item['pubDate'].split(',')[-1]\n",
    "            news_item['pubDate'] = news_item['pubDate'].split(' ')\n",
    "            dia = news_item['pubDate'][1]\n",
    "            mes = news_item['pubDate'][2]\n",
    "            anio = news_item['pubDate'][3]\n",
    "            news_item['pubDate'] = (dia + \"-\" +  mes + \"-\" + anio)\n",
    "            news_item['pubDate'] = MesNumero(news_item['pubDate'])\n",
    "            news_item['category']= item.category.text\n",
    "            variable_des = ()\n",
    "            news_item['link'] = item.link.text\n",
    "            r = requests.get(news_item['link'], headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r.text)\n",
    "            if r.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.td-post-content.tagdiv-type p::text, div.td-post-content.tagdiv-type strong::text\").extract()            \n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des = ''.join(valor)\n",
    "                else:\n",
    "                    variable_des = ''.join(descripcion_larga)\n",
    "            else:\n",
    "                variable_des = ''.join('Error, pagina no encontrada')\n",
    "\n",
    "            news_item['description_long'] = variable_des\n",
    "            news_item['medium']=\"La Guajira Hoy\"\n",
    "            #--  Predict\n",
    "            vect_text1 = news_item['description']\n",
    "            news_item['predi_categ'] = predCateg.predict([vect_text1])[0]\n",
    "            news_item['predi_type'] = predType.predict([vect_text1])[0] \n",
    "            news_items.append(news_item)\n",
    "\n",
    "\n",
    "        rss_guajira_hoy = pd.DataFrame(news_items,columns=['title', 'description','description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])   \n",
    "        return rss_guajira_hoy\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "guajirahoy_guajira = fuentes_guajira_hoy('https://laguajirahoy.com/seccion/la-guajira/feed')\n",
    "guajirahoy_riohacha = fuentes_guajira_hoy('https://laguajirahoy.com/seccion/riohacha/feed')\n",
    "guajirahoy_judicial = fuentes_guajira_hoy('https://laguajirahoy.com/seccion/judiciales/feed')\n",
    "\n",
    "if guajirahoy_guajira is None and guajirahoy_riohacha is None and guajirahoy_judicial is None:\n",
    "    df_diarionorte = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_diarionorte = pd.concat([guajirahoy_guajira, guajirahoy_riohacha,guajirahoy_judicial])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Pilon\n",
    "def fuentes_pilon (periodico_url):\n",
    "    try:\n",
    "    \n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)  \n",
    "        if r_noticias.status_code==200:\n",
    "\n",
    "            titulo = sel.css(\"h4>a::text\").extract()\n",
    "            descripcion = sel.css('div p::text').extract()\n",
    "            descripcion_corregida = limpieza_descripcion(descripcion)\n",
    "            descripcion_corregida = [i.replace('\\n',' ') for i in descripcion_corregida]\n",
    "            fecha = sel.css('li span ::text').extract()\n",
    "            fecha = [i.replace(', ', ' ') for i in fecha]\n",
    "            fecha = [i.replace(' ', '-') for i in fecha]\n",
    "            fecha = [MesNumero(i) for i in fecha]\n",
    "            categoria = sel.css('section h1::text').extract()\n",
    "            categoria = list(map(lambda d: d.replace('\\n        ', ''), categoria))[0]\n",
    "            url = sel.css(\"h4>a ::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://elpilon.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://elpilon.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.land-see-story-copy.max-width-3.mx-auto.py2.px2.ContenidoSingle p ::text, div.land-see-story-copy.max-width-3.mx-auto.py2.px2.ContenidoSingle strong ::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion_corregida:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {} \n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= descripcion_corregida\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['pubDate'] = fecha\n",
    "            noticias['category']= categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium']=\"El Pilon\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            df_pilon = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_pilon\n",
    "\n",
    "        else:\n",
    "            print(periodico_url) \n",
    "            \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "            \n",
    "fuente_judicial = fuentes_pilon('https://elpilon.com.co/Noticias/judicial/')\n",
    "fuente_valledupar = fuentes_pilon('https://elpilon.com.co/Noticias/valledupar/')\n",
    "fuente_cesar = fuentes_pilon('https://elpilon.com.co/Noticias/cesar-y-la-guajira/cesar/')\n",
    "fuente_guajira = fuentes_pilon('https://elpilon.com.co/Noticias/cesar-y-la-guajira/la-guajira/')\n",
    "fuente_municipios = fuentes_pilon('https://elpilon.com.co/Noticias/cesar-y-la-guajira/municipios/')\n",
    "fuente_sur = fuentes_pilon('https://elpilon.com.co/Noticias/cesar-y-la-guajira/sur/')\n",
    "\n",
    "if fuente_judicial is None and fuente_valledupar is None and fuente_cesar is None and fuente_guajira is None and fuente_municipios is None and fuente_sur is None:\n",
    "    df_noticias_pilon = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_noticias_pilon = pd.concat([fuente_judicial, fuente_valledupar, fuente_cesar, fuente_guajira, fuente_municipios, fuente_sur])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diario del Huila\n",
    "def fuentes_huila (periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text) \n",
    "\n",
    "        titulo = sel.css(\"h2.dg_bm_title a::text\").extract()\n",
    "        titulo = [i.strip('\\n') for i in titulo][0:-5]\n",
    "        titulo = [i.replace('\\xa0',' ') for i in titulo]\n",
    "        descripcion = sel.css(\"div.post-content p::text\").extract()\n",
    "        descripcion_corregida = limpieza_descripcion(descripcion)[0:-18]\n",
    "        categoria = sel.css('div.et_pb_text_inner::text').extract()\n",
    "        categoria = [i.lstrip('Estas viendo la sección de') for i in categoria][0]\n",
    "        fecha = sel.css('span.published::text').extract()[0:-5]\n",
    "        fecha_huila = []\n",
    "        for i in fecha:\n",
    "            i = i.replace(', ', ' ')\n",
    "            i = i.replace(' ', '-')\n",
    "            i = i.split('-')\n",
    "            dia = i[0]\n",
    "            mes = i[1]\n",
    "            anio = i[2]\n",
    "            fecha_huila.append(MesNumero(dia + \"-\" + mes + \"-\" + anio))\n",
    "        url = sel.css(\"h2.dg_bm_title a::attr(href)\").extract()[0:-5]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://diariodelhuila.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://diariodelhuila.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.et_pb_module.et_pb_post_content.et_pb_post_content_0_tb_body p::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "    \n",
    "        if len(titulo) == len(descripcion_corregida):\n",
    "            descripcion_corregida = descripcion_corregida\n",
    "        else:\n",
    "            descripcion_corregida = titulo\n",
    "        for x in descripcion_corregida:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo) == len(descripcion_corregida):   \n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion_corregida\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_huila\n",
    "            noticias['medium']=\"Diario El Huila\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_huila\n",
    "            noticias['medium']=\"Diario El Huila\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            \n",
    "        df_huila = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium', 'processDate', 'predi_type', 'predi_categ'])\n",
    "        return df_huila\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_judicial = fuentes_huila(\"https://diariodelhuila.com/judicial/\")\n",
    "fuente_neiva = fuentes_huila(\"https://diariodelhuila.com/neiva/\")\n",
    "fuente_regional = fuentes_huila(\"https://diariodelhuila.com/regional/\")\n",
    "\n",
    "if fuente_judicial is None and fuente_neiva is None and fuente_regional is None:\n",
    "    df_noticias_huila = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDat': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'], 'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_noticias_huila = pd.concat([fuente_judicial, fuente_neiva, fuente_regional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prensa Libre\n",
    "def fuentes_prensalibre (periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias = requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r_noticias.content, 'html.parser')\n",
    "        sel=Selector(r_noticias.text) \n",
    "\n",
    "        titulo=soup.findAll('h2', class_='title')\n",
    "        titulo=[i.get_text().strip('\\n') for i in titulo]\n",
    "        descripcion=soup.findAll('div', class_='clrfix')\n",
    "        descripcion=[i.get_text().strip('\\n') for i in descripcion][1:-2]\n",
    "        descripcion = limpieza(descripcion)\n",
    "        categoria =soup.findAll('div', class_='pr-post-info')\n",
    "        categoria =[i.get_text().lower().strip().split(' |')[-1].replace(' ','') for i in categoria]\n",
    "        enlace =soup.findAll('div', class_='pr-post-info')\n",
    "        enlace=[i.get_text().lower().strip().split(' |')[-1].split(' /')[-1].replace(' ','') for i in enlace][0]\n",
    "        enlace = \"https://prensalibrecasanare.com/\" + enlace\n",
    "        fecha =soup.findAll('div', class_='pr-post-info')\n",
    "        fecha =[i.get_text().strip().split(',')[0] for i in fecha]\n",
    "        fecha_prensa = []\n",
    "        for i in fecha:\n",
    "            if i == 'Hoy':\n",
    "                fecha_prensa.append(dt_string)\n",
    "            elif i == 'Ayer':\n",
    "                delta = timedelta(days=1)      \n",
    "                ayer = now - delta\n",
    "                ayer = ayer.strftime('%d/%m/%Y')\n",
    "                fecha_prensa.append(ayer)\n",
    "            else:\n",
    "                i = i.split('-') \n",
    "                dia = i[0]\n",
    "                mes = i[1]\n",
    "                anio = i[2]\n",
    "                fecha_prensa.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "                \n",
    "        url = sel.css(\"h2 a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://prensalibrecasanare.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://prensalibrecasanare.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.pr-story p::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias ={}    \n",
    "        noticias['title'] = titulo\n",
    "        noticias['description']= descripcion\n",
    "        noticias['description_long']= variable_des\n",
    "        noticias['category']= categoria\n",
    "        noticias['link'] =  variable_url\n",
    "        noticias['pubDate'] = fecha_prensa\n",
    "        noticias['medium']= 'Prensa Libre Casanare'\n",
    "        #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_libre = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        df_libre= df_libre.replace('', 'Noticia sin descripción')\n",
    "        return df_libre\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "\n",
    "fuente_colombia = fuentes_prensalibre('https://prensalibrecasanare.com/colombia/')\n",
    "fuente_judicial =fuentes_prensalibre('https://prensalibrecasanare.com/judicial/')\n",
    "fuente_yopal =fuentes_prensalibre('https://prensalibrecasanare.com/yopal/')\n",
    "fuente_casanare =fuentes_prensalibre('https://prensalibrecasanare.com/casanare/')\n",
    "\n",
    "if fuente_colombia is None and fuente_judicial is None and fuente_yopal is None and fuente_casanare is None:\n",
    "    df_noticias_PrensaLibre = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_noticias_PrensaLibre = pd.concat([fuente_colombia, fuente_judicial, fuente_yopal, fuente_casanare])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-headset",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extra - Judicial- Nombre Periodico: Mi Curillo Caqueta\n",
    "def fuentes_extra (periodico_url):\n",
    "\n",
    "    r_noticias=requests.get(periodico_url, verify=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    sel=Selector(r_noticias.text)\n",
    "    if r_noticias.status_code==200:\n",
    "        try:\n",
    "            titulo = sel.css(\"h2 >a::text\").extract()\n",
    "            titulo = [i.replace('[VIDEO] ','')for i in titulo]\n",
    "            categoria = 'Judicial'\n",
    "            url = sel.css(\"h2 >a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            fecha_extra_m = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://extra.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://extra.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.body-noticia span::text, div.body-noticia p::text, div.body-noticia strong::text, div.body-noticia em::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    fecha_extra =  sel.css(\"p.date::text\").extract()[1]\n",
    "                    fecha_extra = [fecha_extra]\n",
    "                    for i in fecha_extra:\n",
    "                        i = i.split(' /')[0]\n",
    "                        i = i.replace(' de ','-')\n",
    "                        i = i.split('-')\n",
    "                        dia = i[0]\n",
    "                        mes = i[1]\n",
    "                        anio = i[2]\n",
    "                        fecha_extra_m.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            if len(fecha_extra_m) == len(variable_url):\n",
    "                fecha_extra_m\n",
    "            else:\n",
    "                fecha_extra_m = dt_string\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in titulo:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}    \n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= titulo\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['category']= categoria\n",
    "            noticias['link'] =  variable_url\n",
    "            noticias['pubDate'] = fecha_extra_m\n",
    "            noticias['medium']= 'Extra Caqueta'\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            \n",
    "            extra_judicial = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return extra_judicial\n",
    "\n",
    "        except ValueError:\n",
    "            df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "    else:\n",
    "        df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "    \n",
    "extra_judicial = fuentes_extra('https://extra.com.co/judicial')\n",
    "    \n",
    "if extra_judicial is None:\n",
    "    df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_extra_judicial = extra_judicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La Tarde - url El Diario\n",
    "def fuentes_eldiario (periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias = requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        soup = BeautifulSoup(r_noticias.content, 'html.parser')\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo=soup.findAll('h2', class_='entry-title')\n",
    "        titulo=[i.get_text() for i in titulo]\n",
    "        descripcion=soup.findAll('div', class_='entry-summary')\n",
    "        descripcion=[i.get_text().strip()[0:-153] for i in descripcion]\n",
    "        fecha=soup.findAll('time', class_='entry-date')\n",
    "        fecha=[i.get_text() for i in fecha]\n",
    "        fecha_tarde = []\n",
    "        for i in fecha:\n",
    "            i = i.replace(', ', ' ')\n",
    "            i = i.replace(' ', '-')\n",
    "            i = i.split('-') \n",
    "            dia = i[1]\n",
    "            mes = i[0]\n",
    "            anio = i[2]\n",
    "            fecha_tarde.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "        categoria=soup.findAll('h1', class_='page-title')\n",
    "        categoria=[i.get_text().strip().lower().lstrip('local –') for i in categoria][0]\n",
    "        url = sel.css(\"h2 a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.eldiario.com.co/categoria/noticias{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.eldiario.com.co/categoria/noticias/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.entry-content span::text, div.entry-content p::text, div.entry-content strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo)== len(descripcion):\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] =categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_tarde\n",
    "            noticias['medium'] ='El Diario'\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] = descripcion\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] =categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_tarde\n",
    "            noticias['medium'] ='El Diario'\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "\n",
    "        df_eldiario=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_eldiario\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_pereira =fuentes_eldiario('https://www.eldiario.com.co/categoria/noticias/pereira/')\n",
    "fuente_risaralda =fuentes_eldiario('https://www.eldiario.com.co/categoria/noticias/risaralda/')\n",
    "fuente_judicial =fuentes_eldiario('https://www.eldiario.com.co/categoria/judicial/')\n",
    "\n",
    "if fuente_pereira is None and fuente_risaralda is None and fuente_judicial is None:\n",
    "    df_latarde = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_latarde = pd.concat([fuente_pereira, fuente_risaralda, fuente_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HSB-Noticias \n",
    "def fuentes_hsb(periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "        if r_noticias.status_code==200:\n",
    "\n",
    "            titulo = sel.css(\"h2 a::text\").extract()\n",
    "            titulo = [i.replace('[VIDEO] ','')for i in titulo]\n",
    "            categoria = sel.css(\"h2 strong::text\").extract()[0]\n",
    "            url = sel.css(\"h2 a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            fecha_hsb_m = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://hsbnoticias.com{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://hsbnoticias.com/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.body-noticia span::text, div.body-noticia p::text, div.body-noticia strong::text, div.body-noticia em::text\").extract() \n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    fecha_hsb =  sel.css(\"p.date::text\").extract()[1]\n",
    "                    fecha_hsb = [fecha_hsb]\n",
    "                    for i in fecha_hsb:\n",
    "                        i = i.split(' /')[0]\n",
    "                        i = i.replace(' de ','-')\n",
    "                        i = i.split('-')\n",
    "                        dia = i[0]\n",
    "                        mes = i[1]\n",
    "                        anio = i[2]\n",
    "                        fecha_hsb_m.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            if len(fecha_hsb_m) == len(variable_url):\n",
    "                fecha_hsb_m\n",
    "            else:\n",
    "                fecha_hsb_m = dt_string        \n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in titulo:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= titulo\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['category']= categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_hsb_m\n",
    "            noticias['medium']=\"HSB - Noticia\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            hsb = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return hsb\n",
    "        \n",
    "        else:\n",
    "            print(periodico_url)\n",
    "\n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "    \n",
    "hsb_nacional = fuentes_hsb('https://hsbnoticias.com/nacional')\n",
    "hsb_bogota = fuentes_hsb('https://hsbnoticias.com/bogota')\n",
    "hsb_judicial = fuentes_hsb('https://hsbnoticias.com/judicial')\n",
    "\n",
    "if hsb_nacional is None and hsb_bogota is None and hsb_judicial is None:\n",
    "    df_hsb_noticias = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_hsb_noticias = pd.concat([hsb_nacional, hsb_bogota, hsb_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diario del Sur \n",
    "def fuentes_diario_sur(periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"span.field.field--name-title.field--type-string.field--label-hidden::text, div.row div.views-field.views-field-title span.field-content a::text\").extract()\n",
    "        categoria = periodico_url.split('/')[-1]\n",
    "        dia = sel.css(\"div.created-day::text\").extract()\n",
    "        mes = sel.css(\"div.created-month::text\").extract()\n",
    "        anio = sel.css(\"div.created-year::text\").extract()\n",
    "        anio = [i.replace('/', '') for i in anio]\n",
    "        fecha = list(zip(dia,mes,anio))\n",
    "        fecha_sur = []\n",
    "        for i in fecha:\n",
    "            dia = i[0]\n",
    "            mes = i[1]\n",
    "            anio = i[2]\n",
    "            fecha_sur.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "        fecha_1 = [dt_string]\n",
    "        fecha_sur_1 = fecha_1 + fecha_sur\n",
    "        url = sel.css(\"h2.node__title a::attr(href), div.row div.views-field.views-field-title span.field-content a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://diariodelsur.com.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://diariodelsur.com.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.field-item.even p::text, div.field-item.even strong::text, p span::text, div.field__item p::text, div.field__item strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada') \n",
    "                \n",
    "        if len(fecha_sur_1) == len(url):\n",
    "            fecha_sur_1\n",
    "        else:\n",
    "            fecha_sur_1 = dt_string\n",
    "            \n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "        \n",
    "       \n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description']= titulo\n",
    "        noticias['description_long']= variable_des\n",
    "        noticias['category']= categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['pubDate'] = fecha_sur_1\n",
    "        noticias['medium']=\"Diario del Sur\"\n",
    "        #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        diariosur = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return diariosur\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "sur_judicial = fuentes_diario_sur('https://diariodelsur.com.co/judicial')\n",
    "sur_local = fuentes_diario_sur('https://diariodelsur.com.co/local')\n",
    "sur_tumaco = fuentes_diario_sur('https://diariodelsur.com.co/tags/tumaco')\n",
    "sur_ipiales = fuentes_diario_sur('https://diariodelsur.com.co/tags/ipiales')\n",
    "sur_guachucal = fuentes_diario_sur('https://diariodelsur.com.co/tags/guachucal')\n",
    "sur_tuquerres = fuentes_diario_sur('http://diariodelsur.com.co/tags/tuquerres')\n",
    "sur_putumayo = fuentes_diario_sur('https://diariodelsur.com.co/tags/putumayo')\n",
    "\n",
    "if sur_judicial is None and sur_local is None and sur_tumaco is None and sur_ipiales is None and sur_guachucal is None and sur_tuquerres is None and  sur_putumayo is None:\n",
    "    df_diario_del_sur = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_diario_del_sur = pd.concat([sur_judicial, sur_local, sur_tumaco, sur_ipiales, sur_guachucal, sur_tuquerres, sur_putumayo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra - Judicial- Nombre Periodico: Mi Curillo Caqueta\n",
    "def fuentes_extra (periodico_url):\n",
    "\n",
    "    r_noticias=requests.get(periodico_url, verify=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    sel=Selector(r_noticias.text)\n",
    "    if r_noticias.status_code==200:\n",
    "        try:\n",
    "            titulo = sel.css(\"h2 >a::text\").extract()\n",
    "            titulo = [i.replace('[VIDEO] ','')for i in titulo]\n",
    "            categoria = 'Judicial'\n",
    "            url = sel.css(\"h2 >a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            fecha_extra_m = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://extra.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://extra.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.body-noticia span::text, div.body-noticia p::text, div.body-noticia strong::text, div.body-noticia em::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    fecha_extra =  sel.css(\"p.date::text\").extract()[1]\n",
    "                    fecha_extra = [fecha_extra]\n",
    "                    for i in fecha_extra:\n",
    "                        i = i.split(' /')[0]\n",
    "                        i = i.replace(' de ','-')\n",
    "                        i = i.split('-')\n",
    "                        dia = i[0]\n",
    "                        mes = i[1]\n",
    "                        anio = i[2]\n",
    "                        fecha_extra_m.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            if len(fecha_extra_m) == len(variable_url):\n",
    "                fecha_extra_m\n",
    "            else:\n",
    "                fecha_extra_m = dt_string\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in titulo:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}    \n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= titulo\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['category']= categoria\n",
    "            noticias['link'] =  variable_url\n",
    "            noticias['pubDate'] = fecha_extra_m\n",
    "            noticias['medium']= 'Extra Caqueta'\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            \n",
    "            extra_judicial = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return extra_judicial\n",
    "\n",
    "        except ValueError:\n",
    "            df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "            #error_page.append(noticia)\n",
    "\n",
    "    else:\n",
    "        df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "        \n",
    "extra_judicial = fuentes_extra('https://extra.com.co/judicial')\n",
    "    \n",
    "if extra_judicial is None:\n",
    "    df_extra_judicial = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_subtype': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_extra_judicial = extra_judicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mi Putumayo\n",
    "\n",
    "def fuentes_putumayo(periodico_url):\n",
    "    r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    sel=Selector(r_noticias.text) \n",
    "    if r_noticias.status_code==200:\n",
    "        try:\n",
    "            descripcion = sel.css(\"h2.entry-title a::text, h2.entry-title a + br ::text\").extract()\n",
    "            descripcion = [i.strip('br') for i in descripcion]\n",
    "            fecha = sel.css(\"time.entry-date ::text\").extract()\n",
    "            fecha_putumayo = []\n",
    "            for i in fecha:\n",
    "                    i = i.replace(', ', ' ')\n",
    "                    i = i.replace(' ', '-')\n",
    "                    i = i.split('-') \n",
    "                    dia = i[0]\n",
    "                    mes = i[1]\n",
    "                    anio = i[2]\n",
    "                    fecha_putumayo.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "            url = sel.css(\"h2.entry-title a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'http://miputumayo.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'http://miputumayo.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.entry-content p::text, div.entry-content strong::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r ',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = descripcion\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = fecha_putumayo\n",
    "            noticias['category']=\"Noticias Generales\"\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium']=\"Mi Putumayo\"\n",
    "             #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "            putumayo = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return putumayo\n",
    "\n",
    "        except ValueError:\n",
    "            df_putumayo = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "    else:\n",
    "        df_putumayo = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "        \n",
    "putumayo = fuentes_putumayo('http://miputumayo.com.co/author/MiPutumayo/')\n",
    "    \n",
    "if putumayo is None:\n",
    "    df_putumayo = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_putumayo = putumayo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La Cronica del Quindío \n",
    "def fuentes_cronica (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        descripcion = sel.css(\"h2 ::text\").extract()\n",
    "        categoria = sel.css(\"h1 span ::text\").extract()[0].lower()\n",
    "        url = sel.css(\"h2 a::attr(href), div.caption a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.cronicadelquindio.com/noticias{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.cronicadelquindio.com/noticias/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200: \n",
    "                descripcion_larga = sel.css(\"div.col-lg-8 p::text, div.col-lg-8 strong::text\").extract()\n",
    "                if descripcion_larga[0] == 'Acceda sin restricciones a todos nuestros contenidos digitales':\n",
    "                    descripcion_larga = 'Se debe registrarse en la pagina'    \n",
    "                else:\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = descripcion\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = dt_string\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium']=\"La Cronica Del Quindío\"\n",
    "         #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_cronica=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_cronica\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_judicial = fuentes_cronica('https://www.cronicadelquindio.com/noticias/judicial')\n",
    "fuente_region = fuentes_cronica('https://www.cronicadelquindio.com/noticias/region')\n",
    "\n",
    "if fuente_judicial is None and fuente_region is None:\n",
    "    df_cronica_quindio = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_cronica_quindio = pd.concat([fuente_judicial, fuente_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-device",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Policia Nacional\n",
    "def fuentes_policia (periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, verify=False, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            titulo = sel.css(\"div.views-field.views-field-title a::text, span.views-field.views-field-title a::text\").extract()\n",
    "            descripcion = sel.css(\"div.views-field.views-field-field-resumen span.field-content ::text\").extract()\n",
    "            descripcion_corregida = limpieza_descripcion(descripcion)\n",
    "            categoria = 'Noticias Policia'\n",
    "            fecha = sel.css(\"div.views-field.views-field-created span.field-content ::text\").extract()\n",
    "            fecha_policia = []\n",
    "            for i in fecha:\n",
    "                i = i.split(', ')[-1]\n",
    "                i = i.replace(' de ','-')\n",
    "                i = i.split('-') \n",
    "                dia = i[0]\n",
    "                mes = i[1]\n",
    "                anio = i[2]\n",
    "                fecha_policia.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "            url = sel.css(\"div.views-field.views-field-title a::attr(href), span.views-field.views-field-title a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.policia.gov.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.policia.gov.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div p::text, div strong::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            if len(titulo) == len(descripcion):\n",
    "                descripcion_corregida = descripcion\n",
    "            else:\n",
    "                descripcion_corregida = titulo           \n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion_corregida:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            if len(titulo) == len(descripcion): \n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description'] = descripcion_corregida\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['category'] = categoria \n",
    "                noticias['link'] = variable_url\n",
    "                noticias['pubDate'] = fecha_policia\n",
    "                noticias['medium']=\"Policia Nacional\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "            else:\n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description'] = titulo\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['category'] = categoria \n",
    "                noticias['link'] = variable_url\n",
    "                noticias['pubDate'] = fecha_policia\n",
    "                noticias['medium']=\"Policia Nacional\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "                \n",
    "            policia = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return policia\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print ('Timeout occurred')\n",
    "        \n",
    "    except ValueError:\n",
    "        df_policia = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "\n",
    "policia = fuentes_policia('https://www.policia.gov.co/noticias')\n",
    "    \n",
    "if policia is None:\n",
    "    df_policia = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_policia = policia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-binary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unidad de Victimas \n",
    "def fuentes_unidad(periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, verify=False, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            descripcion = sel.css(\"h5.tituloh5 a::text\").extract()\n",
    "            categoria = sel.css(\"h6.ciudadfecha::text\").extract()\n",
    "            categoria = [i.split(' |')[0] for i in categoria]\n",
    "            fecha = sel.css(\"h6.ciudadfecha::text\").extract()\n",
    "            fecha_unidad = []\n",
    "            for i in fecha:\n",
    "                a = dt_string.split('/')\n",
    "                i = i.split(', ')[-1]\n",
    "                i = i.replace(' de ','-')\n",
    "                i = i.split('-') \n",
    "                dia = i[0]\n",
    "                mes = i[1]\n",
    "                anio = a[2]\n",
    "                fecha_unidad.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "            url = sel.css(\"h5.tituloh5 a::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, verify=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.unidadvictimas.gov.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, verify=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.unidadvictimas.gov.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, verify=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.row p::text, div.row strong::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = descripcion\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['pubDate'] = fecha_unidad\n",
    "            noticias['medium'] = 'Unidad de Victimas'\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "            \n",
    "            unidad_victimas = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return unidad_victimas\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print ('Timeout occurred')\n",
    "\n",
    "    except ValueError:\n",
    "        df_unidadvictimas = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "unidadvictimas = fuentes_unidad('https://www.unidadvictimas.gov.co/es/sala-de-prensa/noticias')\n",
    "\n",
    "if unidadvictimas is None:\n",
    "    df_unidadvictimas = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_unidadvictimas = unidadvictimas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-beginning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fuerza Aerea Colombiana \n",
    "def fuentes_fuerza(periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            url = sel.css(\"div.post-title a::attr(href)\").extract()\n",
    "            url = pd.DataFrame(url)\n",
    "            url = url.drop_duplicates([0], keep='first')\n",
    "            url = url.values.tolist()\n",
    "            url = [item for lista in url for item in lista]\n",
    "            l = len(url)\n",
    "            titulo = sel.css(\"div.post-title a::text\").extract()[:l]\n",
    "            descripcion = sel.css(\"div.body p::text\").extract()\n",
    "            descripcion = [e for e in descripcion if e not in ('\\xa0', '\\n', '\\r')]\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            fecha_fuerza = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.fac.mil.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.fac.mil.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"p.MsoNormal span::text, div.field.field--name-body.field--type-text-with-summary.field--label-hidden.field__item p::text, div.field.field--name-body.field--type-text-with-summary.field--label-hidden.field__item strong::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    fecha = sel.css(\"div.content-field.field-created::text\").extract()\n",
    "                    for i in fecha:\n",
    "                        i = i.replace('\\n','').replace(' ','').replace('de','-')\n",
    "                        i = i.split('-')\n",
    "                        dia = i[0]\n",
    "                        mes = i[1]\n",
    "                        anio = i[2]\n",
    "                        fecha_fuerza.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            if len(fecha_fuerza) == len(variable_url):\n",
    "                fecha_fuerza\n",
    "            else:\n",
    "                fecha_fuerza = dt_string\n",
    "\n",
    "            if len(titulo) == len(descripcion):\n",
    "                descripcion = descripcion\n",
    "            else:\n",
    "                descripcion = titulo\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            if len(titulo) == len(descripcion):\n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description'] = descripcion\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['category']= 'Información y prensa'\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['medium'] = \"Fuerza Aerea Colombiana\"\n",
    "                noticias['pubDate'] = fecha_fuerza\n",
    "\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego \n",
    "            else:\n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description'] = titulo\n",
    "                noticias['description_long'] = variable_des\n",
    "                noticias['category']= 'Información y prensa'\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['medium'] = \"Fuerza Aerea Colombiana\"\n",
    "                noticias['pubDate'] = fecha_fuerza\n",
    "\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "            \n",
    "            fuerza = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return fuerza\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print ('Timeout ocurred')\n",
    "\n",
    "    except ValueError:\n",
    "        df_fuerza =  pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "        error_page.append(noticia)\n",
    "        \n",
    "fuerza = fuentes_fuerza('https://www.fac.mil.co/es/noticias')\n",
    "\n",
    "if fuerza is None:\n",
    "    df_fuerza  = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_fuerza = fuerza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-indian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejercito\n",
    "def fuentes_ejercito(periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            titulo = sel.css(\"h5>a::text\").extract()\n",
    "            descripcion = sel.css(\"p.listaEntradilla ::text\").extract()\n",
    "            descripcion = limpieza_descripcion(descripcion)\n",
    "            if r_noticias.status_code==200:\n",
    "                categoria = sel.css(\"div.titulo-interna::text\").extract()[0]\n",
    "            else:\n",
    "                categoria = 'Ejercito'\n",
    "            url = sel.css(\"h5>a.wrap8::attr(href)\").extract()\n",
    "            fecha = sel.css(\"p.s_fecha::text\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            fecha_ejercito = []\n",
    "            for i in fecha:\n",
    "                dia = i[0:2]\n",
    "                dia = dia.replace(' ','')\n",
    "                i = i.split(' de ')[-1].replace(' ','-').split('-')\n",
    "                mes = i[0]\n",
    "                anio= i[1]\n",
    "                fecha_ejercito.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.ejercito.mil.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.ejercito.mil.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.default_descripcion.row::text\").extract()\n",
    "                    descripcion_larga = limpieza_descripcion(descripcion_larga)\n",
    "                    descripcion_larga = [i.replace('\\n','') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t','') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')   \n",
    "            catego = []\n",
    "            tipo = []\n",
    "            if len(titulo) == len(descripcion):\n",
    "                descripcion = descripcion\n",
    "            else:\n",
    "                descripcion = titulo\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type) \n",
    "\n",
    "            if len(titulo) == len(descripcion): \n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description']= descripcion\n",
    "                noticias['description_long']= variable_des\n",
    "                noticias['category']= categoria\n",
    "                noticias['pubDate'] = fecha_ejercito\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['medium']=\"Ejercito Militar\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "\n",
    "            else: \n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description']= titulo\n",
    "                noticias['description_long']= variable_des\n",
    "                noticias['category']= categoria\n",
    "                noticias['pubDate'] = fecha_ejercito\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['medium']=\"Ejercito Militar\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego\n",
    "\n",
    "            df_militar = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_militar\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print ('Timeout ocurred')\n",
    "            \n",
    "    except ValueError:\n",
    "        df_ejercito = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "ejercito_noticias = fuentes_ejercito(\"https://www.ejercito.mil.co/informes_noticias/noticias\")\n",
    "ejercito_actualidad = fuentes_ejercito(\"https://www.ejercito.mil.co/informes_noticias/actualidad\")\n",
    "\n",
    "if ejercito_noticias is None and ejercito_actualidad is None:\n",
    "    df_ejercito = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_ejercito = pd.concat([ejercito_noticias, ejercito_actualidad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-batch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Armada\n",
    "def fuentes_armada (periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, timeout=20, allow_redirects=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            descripcion = sel.css(\"span>a::text\").extract()[:8]\n",
    "            url = sel.css(\"span.field-content a::attr(href)\").extract()[:8]\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, timeout=20, allow_redirects=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.armada.mil.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, timeout=20, allow_redirects=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.armada.mil.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, timeout=20, allow_redirects=False, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.field-item.even p::text\").extract()\n",
    "                    descripcion_larga = limpieza_descripcion(descripcion_larga)\n",
    "                    descripcion_larga = [i.replace('\\n','') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t','') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = descripcion\n",
    "            noticias['description']= descripcion\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['category']= 'Armada'\n",
    "            noticias['pubDate'] = dt_string\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium']=\"Fuerzas Armadas de Colombia\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego \n",
    "\n",
    "            armada = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return armada\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            print ('Timeout occurred')\n",
    "        \n",
    "    except ValueError:\n",
    "        df_armada = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "armada = fuentes_armada(\"https://www.armada.mil.co/\")\n",
    "\n",
    "if armada is None:\n",
    "    df_armada = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_armada = armada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-productivity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# El Tiempo\n",
    "\n",
    "def fuentes_tiempo(periodico_url):\n",
    "    try:\n",
    "        r_noticias = requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "        soup = BeautifulSoup(r_noticias.content, 'html.parser')\n",
    "\n",
    "        descripcion = soup.findAll('div', class_='epigraph-container')\n",
    "        descripcion = [i.get_text().strip('\\n') for i in descripcion]\n",
    "        l = len(descripcion)\n",
    "        titulo = soup.findAll('h3', class_='title-container')\n",
    "        titulo = [i.get_text().strip('\\n') for i in titulo][:l]\n",
    "        fecha = sel.css(\"span.published-at::text\").extract()[:l]\n",
    "        fecha_tiempo = []\n",
    "        for i in fecha:\n",
    "            variable = i[6:10]\n",
    "            if variable =='a. m':\n",
    "                fecha_tiempo.append(dt_string)\n",
    "            elif variable =='p. m':\n",
    "                delta = timedelta(days=1)\n",
    "                ayer = now - delta\n",
    "                ayer = ayer.strftime('%d/%m/%Y')\n",
    "                fecha_tiempo.append(ayer)\n",
    "            elif variable !='a. m' or variable !='p. m':\n",
    "                i = i.split(' ')\n",
    "                dia = i[1]\n",
    "                mes = i[0]\n",
    "                anio = i[3]\n",
    "                fecha_tiempo.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "        url = sel.css(\"div.epigraph-container a.epigraph.page-link::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.eltiempo.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.eltiempo.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.modulos.public-side p::text, div.modulos.public-side b::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0','') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    variable = 'sin descripcion en la noticia'\n",
    "                    variable_des.append(variable)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        categoria = soup.findAll('h1', class_='oculto')\n",
    "        categoria = [i.get_text() for i in categoria][0]\n",
    "        enlace = periodico_url\n",
    "        medio = \"El Tiempo\"  \n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description']= descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['category']= categoria\n",
    "        noticias['pubDate'] = fecha_tiempo\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium']= medio\n",
    "        #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        df = df.replace('', 'Noticia sin descripción')\n",
    "        df = df.replace('\\n', '-',regex=True) \n",
    "        return df\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "def fuentes_tiempo_2 (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css('h3>a::text, h2>a::text').extract()\n",
    "        titulo = [i.replace('\\n','')for i in titulo]\n",
    "        descripcion = sel.css('h3>a::text, div.lead a::text').extract()\n",
    "        descripcion = limpieza_descripcion(descripcion)\n",
    "        categoria = sel.css(\"h1.titulo ::text\").extract()[0]\n",
    "        fecha = sel.css(\"div.modulo-mas-default meta, div.seccion-fecha meta, div.listing.view-grid meta\").extract()\n",
    "        fecha = [i.split() for i in fecha]\n",
    "        fecha = pd.DataFrame(fecha)\n",
    "        fecha = fecha.drop(fecha[fecha[1]!='itemprop=\"datePublished\"'].index)\n",
    "        fecha = fecha[2]\n",
    "        fecha = [i.split('=')[-1][1:-2] for i in fecha]\n",
    "        fecha_tiempo = []\n",
    "        for i in fecha:\n",
    "            i = i.split('-') \n",
    "            dia = i[2]\n",
    "            mes = i[1]\n",
    "            anio = i[0]\n",
    "            fecha_tiempo.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "        url = sel.css('h3>a::attr(href), div.lead a::attr(href)').extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.eltiempo.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.eltiempo.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.modulos.public-side p::text, div.modulos.public-side b::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0','') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('⚠️','') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    variable = 'sin descripcion en la noticia'\n",
    "                    variable_des.append(variable)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)         \n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0] \n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description']= descripcion\n",
    "        noticias['description_long']= variable_des\n",
    "        noticias['category']= categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['pubDate'] = fecha_tiempo\n",
    "        noticias['medium']=\"El Tiempo\"\n",
    "        #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        df = df.replace('\\n', '-',regex=True)\n",
    "        return df\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "def fuente_bogota(periodico_url):\n",
    "    try:\n",
    "        r_noticias = requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"section.col0 h3.title-container a::text\").extract()\n",
    "        titulo = [i.strip('\\n') for i in titulo]\n",
    "        titulo = pd.DataFrame(titulo)\n",
    "        titulo = titulo.replace('', 'NaN')\n",
    "        titulo = titulo.drop(titulo[titulo[0]=='NaN'].index)\n",
    "        titulo = titulo.values.tolist()\n",
    "        titulo = [item for lista in titulo for item in lista]\n",
    "        l = len(titulo)\n",
    "        descripcion = titulo\n",
    "        fecha = sel.css(\"section.col0 div meta\").extract()\n",
    "        fecha = [i.split() for i in fecha]\n",
    "        fecha = pd.DataFrame(fecha)\n",
    "        fecha = fecha.drop(fecha[fecha[1]!='itemprop=\"datePublished\"'].index)\n",
    "        fecha = fecha[2]\n",
    "        fecha = [i.split('=')[-1][1:-2] for i in fecha][:l]\n",
    "        fecha_tiempo = []\n",
    "        for i in fecha:\n",
    "            i = i.split('-') \n",
    "            dia = i[2]\n",
    "            mes = i[1]\n",
    "            anio = i[0]\n",
    "            fecha_tiempo.append(dia + \"/\" +  mes + \"/\" + anio) \n",
    "        url = sel.css(\"section.col0 a.title.page-link ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.eltiempo.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.eltiempo.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                    \n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.modulos.public-side p::text, div.modulos.public-side b::text, div.lead::text, div.wpb_wrapper p::text, div.wpb_wrapper strong::text, div.wpb_wrapper em::text, div.wpb_wrapper a::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0','') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    variable = 'sin descripcion en la noticia'\n",
    "                    variable_des.append(variable)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga) \n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        categoria = 'Bogota'\n",
    "        medio = \"El Tiempo\"       \n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description']= titulo\n",
    "        noticias['description_long']= variable_des\n",
    "        noticias['category']= categoria\n",
    "        noticias['pubDate'] = fecha_tiempo\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium']= medio\n",
    "        #--  Predict\n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        df = df.replace('\\n', '-',regex=True)\n",
    "        return df\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_bogota = fuente_bogota('https://www.eltiempo.com/bogota')\n",
    "fuente_medellin = fuentes_tiempo('https://www.eltiempo.com/colombia/medellin')\n",
    "fuente_otras_ciudades = fuentes_tiempo('https://www.eltiempo.com/colombia/otras-ciudades')\n",
    "fuente_santander = fuentes_tiempo('https://www.eltiempo.com/colombia/santander')\n",
    "fuente_barranquilla = fuentes_tiempo('https://www.eltiempo.com/colombia/barranquilla')\n",
    "fuente_cali = fuentes_tiempo('https://www.eltiempo.com/colombia/cali')\n",
    "fuente_boyaca = fuentes_tiempo_2('https://www.eltiempo.com/noticias/boyaca')\n",
    "fuente_meta = fuentes_tiempo_2('https://www.eltiempo.com/noticias/meta')\n",
    "\n",
    "if fuente_bogota is None and fuente_medellin is None and fuente_otras_ciudades is None and fuente_santander is None and fuente_barranquilla is None and fuente_cali is None and fuente_boyaca is None and fuente_meta is None:\n",
    "    df_noticias_ElTiempo = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_noticias_ElTiempo = pd.concat([fuente_bogota, fuente_medellin, fuente_otras_ciudades, fuente_santander, fuente_barranquilla, fuente_cali, fuente_boyaca, fuente_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La voz del Cinaruco\n",
    "def fuentes_cinaruco (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h2 a::text ,h3 a ::text\").extract()\n",
    "        descripcion = sel.css('article.col-xs-12.col-sm-4.col-md-4 p, div.well p').extract()\n",
    "        descripcion = limpieza(descripcion)\n",
    "        descripcion = [i.replace('<p style=\"text-align: justify;\">','') for i in descripcion]\n",
    "        descripcion = [i.replace('</strong>','') for i in descripcion]\n",
    "        descripcion = [i.replace('<strong>','') for i in descripcion]\n",
    "        descripcion = [i.replace('</p>','') for i in descripcion]\n",
    "        descripcion = [i.replace('<p>','') for i in descripcion]\n",
    "        descripcion = pd.DataFrame(descripcion)\n",
    "        descripcion = descripcion.drop(descripcion[descripcion[0]==''].index)\n",
    "        descripcion = descripcion.values.tolist()\n",
    "        descripcion = [item for lista in descripcion for item in lista]\n",
    "        url = sel.css(\"h2 a::attr(href) ,h3 a ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://lavozdelcinaruco.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://lavozdelcinaruco.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.panel-body p::text, div.panel-body strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == ' ':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        if len(titulo) == len(descripcion):\n",
    "            descripcion = descripcion\n",
    "        else:\n",
    "            descripcion = titulo\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo) == len(descripcion):\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = dt_string\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['category'] = \"Noticias\"\n",
    "            noticias['medium'] = \"La Voz del Cinaruco\"\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = dt_string\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['category'] = \"Noticias\"\n",
    "            noticias['medium'] = \"La Voz del Cinaruco\"\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        cinaruco = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return cinaruco\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "        \n",
    "cinaruco = fuentes_cinaruco(\"https://lavozdelcinaruco.com/\")\n",
    "\n",
    "if cinaruco is None:\n",
    "    df_cinaruco = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_cinaruco = cinaruco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-warrior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# La Patria \n",
    "\n",
    "def fuentes_la_patria (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        categoria = sel.css(\"h1 span ::text\").extract()[0]\n",
    "        descripcion = sel.css(\"div.content_main.griddiv span.field-content a ::text\").extract()\n",
    "        url = sel.css(\"div.content_main.griddiv span.field-content a ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.lapatria.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.lapatria.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.field-item.even p::text, div.field-item.even strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == ' ':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = descripcion\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = dt_string\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium'] = \"La Patria\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_la_patria=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_la_patria\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_manizales = fuentes_la_patria('https://www.lapatria.com/Manizales')\n",
    "fuente_caldas = fuentes_la_patria('https://www.lapatria.com/caldas')\n",
    "fuente_nacional = fuentes_la_patria('https://www.lapatria.com/nacional')\n",
    "\n",
    "if fuente_manizales is None and fuente_caldas is None and fuente_nacional is None:\n",
    "    df_la_patria = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_la_patria = pd.concat([fuente_manizales, fuente_caldas, fuente_nacional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El diario del cauca\n",
    "\n",
    "def fuente_cauca (periodico_url):\n",
    "    r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    sel=Selector(r_noticias.text)\n",
    "    if r_noticias.status_code==200:\n",
    "        try:\n",
    "            l = 10\n",
    "            titulo = sel.css(\"div.views-field.views-field-title a::text\").extract()[:l]\n",
    "            descripcion = sel.css(\"div.views-field.views-field-body div.field-content::text\").extract()[:l]\n",
    "            descripcion = limpieza_descripcion(descripcion)\n",
    "            descripcion = [i.replace('\\n\\n',' ')  for i in descripcion]\n",
    "            descripcion = [i.replace('\\n',' ')  for i in descripcion]\n",
    "            categoria = sel.css(\"h1.page-header::text\").extract()[0]\n",
    "            fecha_cauca = []\n",
    "            fecha = sel.css(\"div.created-comment::text\").extract()[:l]\n",
    "            for i in fecha:\n",
    "                i = i.split(', ')\n",
    "                dia = i[1]      \n",
    "                mes = i[0]\n",
    "                anio = i[2].replace(' ','')\n",
    "                fecha_cauca.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "            url = sel.css(\"div.views-field.views-field-title a::attr(href)\").extract()[:l]\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://diariodelcauca.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://diariodelcauca.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.field.field--name-body.field--type-text-with-summary.field--label-hidden.field--item p::text, div.field.field--name-body.field--type-text-with-summary.field--label-hidden.field--item span::text, div.field.field--name-body.field--type-text-with-summary.field--label-hidden.field--item strong::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0]\n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = fecha_cauca\n",
    "            noticias['category'] = categoria\n",
    "            noticias['medium'] = \"Diario del Cauca\"\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "            df_cauca = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_cauca\n",
    "        \n",
    "        except ValueError:\n",
    "            df_cauca = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "    \n",
    "    else:\n",
    "        df_cauca = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "    \n",
    "fuente_judicial = fuente_cauca(\"https://diariodelcauca.com.co/judicial\")\n",
    "fuente_local = fuente_cauca(\"https://diariodelcauca.com.co/local\")\n",
    "\n",
    "if fuente_local is None and fuente_judicial is None:\n",
    "    df_cauca = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_cauca = pd.concat([fuente_judicial, fuente_local])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-jimmy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# El nuevo liberal\n",
    "driver = webdriver.Chrome(executable_path='C:/Users/harold.patino/Documents/Periodicos/chromedriver.exe', options=options)\n",
    "\n",
    "def fuente_nuevo_liberal(periodico_url):\n",
    "    try:\n",
    "        driver.get(periodico_url)\n",
    "        status = [i.text for i in driver.find_elements_by_tag_name('h1')]\n",
    "        if len(status) > 0:              \n",
    "            titulo = [i.text for i in driver.find_elements_by_tag_name('h2.jeg_post_title, h3.jeg_post_title')]\n",
    "            descripcion = [i.text for i in driver.find_elements_by_tag_name('h2.jeg_post_title, div.jeg_post_excerpt p')]\n",
    "            fecha = [i.text for i in driver.find_elements_by_tag_name('div.jeg_meta_date a')]\n",
    "            fecha_liberal = []\n",
    "            for i in fecha:\n",
    "                if len(i)== 0:\n",
    "                    i = dt_string\n",
    "                else:\n",
    "                    i = i\n",
    "                fecha_liberal.append(i)\n",
    "            url = [i.get_attribute('href') for i in driver.find_elements_by_tag_name('h2.jeg_post_title a, div.jeg_post_excerpt a')]\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    driver.get(variable)\n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://elnuevoliberal.com{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        driver.get(variable)\n",
    "                    else:\n",
    "                        variable = 'https://elnuevoliberal.com/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        driver.get(variable)\n",
    "                if len(titulo)!=0:\n",
    "                    descripcion_larga = [i.text for i in driver.find_elements_by_tag_name('div.content-inner p, div.content-inner strong, div.content-inner em')]\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {} \n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= descripcion\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['pubDate'] = fecha_liberal\n",
    "            noticias['category']= 'Judicial'\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium']=\"El Nuevo Liberal\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "            df_nuevo_liberal = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_nuevo_liberal\n",
    "\n",
    "        else:\n",
    "            df_nuevo_liberal= pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "    \n",
    "    except ValueError:\n",
    "        df_nuevo_liberal= pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "        \n",
    "fuente_judicial = fuente_nuevo_liberal('https://elnuevoliberal.com/category/judicial/')\n",
    "\n",
    "if fuente_judicial is None:\n",
    "    df_nuevo_liberal= pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_nuevo_liberal = fuente_judicial\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-wisdom",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# El Heraldo \n",
    "def fuentes_el_heraldo (periodico_url):\n",
    "    try:\n",
    "        try:\n",
    "            r_noticias=requests.get(periodico_url, timeout=20, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "            sel=Selector(r_noticias.text)\n",
    "\n",
    "            descripcion = sel.css(\"div.text h1 a ::text\").extract()\n",
    "            fecha = sel.css(\"div.datos ::attr(datetime)\").extract()\n",
    "            fecha_heraldo = []\n",
    "            for i in fecha:\n",
    "                i = i.split(' ')[0]\n",
    "                i = i.split('-') \n",
    "                dia = i[2]      \n",
    "                mes = i[1]\n",
    "                anio = i[0]\n",
    "                fecha_heraldo.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "\n",
    "            url = sel.css(\"div.text h1 ::attr(href)\").extract()\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.elheraldo.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.elheraldo.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"div.field-item.even p::text, div.field-item.even strong::text, div.column.center.content-blocked p::text, div.column.center.content-blocked strong::text, div.column right p::text, div.column right strong::text, p.summary ::text, strong.summary::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0]\n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {}\n",
    "            noticias['title'] = descripcion\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['category'] = [i.split('/')[3] for i in variable_url]\n",
    "            noticias['pubDate'] = fecha_heraldo\n",
    "            noticias['medium'] = \"El Heraldo\"\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "            df_elheraldo=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_elheraldo\n",
    "        \n",
    "        except requests.exceptions.RequestException:\n",
    "            print (periodico_url)  \n",
    "\n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_bolivar = fuentes_el_heraldo('https://www.elheraldo.co/bolivar')\n",
    "fuente_cesar = fuentes_el_heraldo('https://www.elheraldo.co/cesar')\n",
    "fuente_cordoba = fuentes_el_heraldo('https://www.elheraldo.co/cordoba')\n",
    "fuente_guajira = fuentes_el_heraldo('https://www.elheraldo.co/la-guajira')\n",
    "fuente_magdalena = fuentes_el_heraldo('https://www.elheraldo.co/magdalena')\n",
    "fuente_sucre = fuentes_el_heraldo('https://www.elheraldo.co/sucre')\n",
    "fuente_san_andres = fuentes_el_heraldo('https://www.elheraldo.co/san-andres')\n",
    "fuente_local = fuentes_el_heraldo('https://www.elheraldo.co/local')\n",
    "fuente_judicial = fuentes_el_heraldo('https://www.elheraldo.co/judicial')\n",
    "\n",
    "\n",
    "if fuente_bolivar is None and fuente_cesar is None and fuente_cordoba is None and fuente_guajira is None and fuente_magdalena is None and fuente_sucre is None and fuente_san_andres is None and fuente_local is None and fuente_judicial is None:\n",
    "    df_el_heraldo = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_el_heraldo = pd.concat([fuente_bolivar, fuente_cesar, fuente_cordoba, fuente_guajira, fuente_magdalena, fuente_sucre, fuente_san_andres, fuente_local, fuente_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RCN\n",
    "headers = {\"content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\", \"DNT\": \"1\", \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36\"}\n",
    "query_params = {\"pageSize\": 32, \"compSeasons\": 274, \"altIds\": True, \"page\": 0, \"type\": \"player\", \"id\": -1, \"compSeasonId\": 274}\n",
    "cs = cloudscraper.create_scraper(browser={'browser': 'firefox','platform': 'windows','mobile': False}, \n",
    "                                 delay=10, interpreter='nodejs')\n",
    "\n",
    "def fuentes_rcn (periodico_url):\n",
    "    try:\n",
    "        \n",
    "        r_noticias = cs.get(periodico_url, headers=headers, params = query_params)\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        descripcion = sel.css(\"div.col-sm-12.col-md-12.col-lg-8.border-right h1 a ::text, div.field.field--name-node-title.field--type-ds.field--label-hidden.field__item h1 a::text\").extract()\n",
    "        descripcion = [i.replace('\\xa0',' ') for i in descripcion]\n",
    "        l = len(descripcion)\n",
    "        categoria = sel.css(\"div.field.field--name-field-section.field--type-entity-reference.field--label-hidden.field__item a::text, div.field.field--name-field-region.field--type-entity-reference.field--label-hidden.field__item a ::text\").extract()[:l]\n",
    "        fecha = sel.css(\"div.flex-section div.field.field--name-field-pub-date.field--type-datetime.field--label-hidden.field__item::text, div.second-date div.field.field--name-field-pub-date.field--type-datetime.field--label-hidden.field__item::text\").extract()\n",
    "        fecha_actual = time_date(fecha)\n",
    "        url = sel.css(\"div.col-sm-12.col-md-12.col-lg-8.border-right h1 a::attr(href), div.field.field--name-node-title.field--type-ds.field--label-hidden.field__item h1 a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=cs.get(variable, headers=headers, params = query_params)\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com{urls}'.format(urls=item)\n",
    "                    variable_visual = 'https://www.rcnradio.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable_visual)\n",
    "                    r_noticias=cs.get(variable, headers=headers, params = query_params)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/{urls}'.format(urls=item)\n",
    "                    variable_visual = 'https://www.rcnradio.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable_visual)\n",
    "                    r_noticias=cs.get(variable, headers=headers, params = query_params)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css('div.item4 p::text, div.item4 strong::text').extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Validar información de la noticia, mediante el link dado en la columna de link')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = descripcion\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['category'] = categoria\n",
    "        noticias['pubDate'] = fecha_actual\n",
    "        noticias['medium'] = \"RCN Radio\"\n",
    "        noticias['link'] = variable_url\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_rcn = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type', 'predi_categ'])\n",
    "        return df_rcn\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_colombia = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia')\n",
    "fuente_bogota = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/bogota')\n",
    "fuente_antioquia = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/antioquia')\n",
    "fuente_sur = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/sur-del-pais')\n",
    "fuente_caribe = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/caribe')\n",
    "fuente_cafetero = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/eje-cafetero')\n",
    "fuente_llanos = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/llanos')\n",
    "fuente_pacifico = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/pacifico')\n",
    "fuente_region_central = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/region-central')\n",
    "fuente_santanderes = fuentes_rcn('http://webcache.googleusercontent.com/search?q=cache:https://www.rcnradio.com/colombia/santanderes')\n",
    "\n",
    "if fuente_colombia is None and fuente_bogota is None and fuente_antioquia is None and fuente_sur is None and fuente_caribe is None and fuente_cafetero is None and fuente_llanos is None and fuente_pacifico is None and fuente_region_central is None and fuente_santanderes is None:\n",
    "    df_rcn_radio = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_rcn_radio = pd.concat([fuente_colombia, fuente_bogota, fuente_antioquia, fuente_sur, fuente_caribe, fuente_cafetero, fuente_llanos, fuente_pacifico, fuente_region_central, fuente_santanderes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-source",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Caracol\n",
    "def fuentes_caracol (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"ul.items.layout-automatic-c  h2.module-title a ::text\").extract()\n",
    "        descripcion = sel.css(\"ul.items.layout-automatic-c p.module-text ::text\").extract()\n",
    "        l = len(titulo)\n",
    "        categoria = sel.css(\"h1.headline strong ::text\").extract()[0]\n",
    "        fecha = sel.css(\"div.module-item-content a::attr(href)\").extract()\n",
    "        fecha = [i.split('/')[-1].split('_')[0]for i in fecha]\n",
    "        fecha = pd.DataFrame(fecha)\n",
    "        fecha = fecha.drop(fecha[fecha[0]==''].index)\n",
    "        fecha = fecha.values.tolist()[:l]\n",
    "        fecha = [item for lista in fecha for item in lista]\n",
    "        fecha = time_date(fecha)\n",
    "        url = sel.css(\"ul.items.layout-automatic-c  h2.module-title a ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://caracol.com.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://caracol.com.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css('div.cuerpo p::text, div.cuerpo strong::text').extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t','') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        if len(titulo) == len(descripcion):\n",
    "            descripcion = descripcion\n",
    "        else:\n",
    "            descripcion = titulo\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)    \n",
    "\n",
    "        if len(titulo)== len(descripcion):\n",
    "            noticias = {}\n",
    "            noticias['title'] =  titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha\n",
    "            noticias['medium'] = \"Caracol Radio\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] =  titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha\n",
    "            noticias['medium'] = \"Caracol Radio\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        df_caracol=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_caracol\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_nacional = fuentes_caracol('https://caracol.com.co/seccion/nacional/')\n",
    "fuente_regional = fuentes_caracol('https://caracol.com.co/seccion/regional/')\n",
    "fuente_judicial = fuentes_caracol('https://caracol.com.co/seccion/judicial/')\n",
    "\n",
    "if fuente_nacional is None and fuente_regional is None and fuente_judicial is None:\n",
    "    df_caracol = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_caracol = pd.concat([fuente_nacional, fuente_regional, fuente_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-editing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colombiano\n",
    "def fuentes_colombiano (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo =  sel.css(\"div.tx-titulo span.priority-content::text, div.noticias-tercer-nivel span.priority-content::text\").extract()\n",
    "        descripcion = sel.css(\"div.tx-titulo span.priority-content::text, div.noticias-tercer-nivel p:last-child::text\").extract()\n",
    "        descripcion = pd.DataFrame(descripcion)\n",
    "        descripcion = descripcion.drop(descripcion[descripcion[0]==' '].index)\n",
    "        descripcion = descripcion.values.tolist()\n",
    "        descripcion = [item for lista in descripcion for item in lista]\n",
    "        categoria = sel.css(\"div.fecha a::text, div.noticias-tercer-nivel div.categoria-noticia a::text\").extract()\n",
    "        fecha = sel.css('div.modulo-principal-seccion div::text, div.noticias-tercer-nivel div::text').extract()\n",
    "        fecha = [i.split('|')[-1].split('\\n')[0].strip(' ') for i in fecha]\n",
    "        fecha = pd.DataFrame(fecha)\n",
    "        fecha = fecha.drop(fecha[fecha[0]==''].index)\n",
    "        fecha = fecha.drop(fecha[fecha[0]=='EXCLUSIVO SUSCRIPTORES'].index)\n",
    "        fecha = fecha.values.tolist()\n",
    "        fecha = [item for lista in fecha for item in lista]\n",
    "        fecha_colombiano = []\n",
    "        for i in fecha:\n",
    "            if i[-2:] == 'AM':\n",
    "                fecha_colombiano.append(dt_string)\n",
    "            elif i[-2:] == 'PM':\n",
    "                delta = timedelta(days=1)\n",
    "                ayer = now - delta\n",
    "                ayer = ayer.strftime('%d/%m/%Y')\n",
    "                fecha_colombiano.append(ayer)\n",
    "            else:\n",
    "                fecha_colombiano.append(i)\n",
    "            \n",
    "        url = sel.css(\"div.foto a::attr(href),div.left a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.elcolombiano.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.elcolombiano.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.block-text p::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t','') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)     \n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "            \n",
    "        if len(titulo) == len(descripcion): \n",
    "            descripcion = descripcion\n",
    "        else:\n",
    "            descripcion = titulo\n",
    "            \n",
    "        if len(titulo) == len(fecha_colombiano):\n",
    "            fecha_colombiano\n",
    "        else:\n",
    "            fecha_colombiano = dt_string\n",
    "            \n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo) == len(descripcion): \n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha_colombiano\n",
    "            noticias['medium'] = \"Colombiano\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha_colombiano\n",
    "            noticias['medium'] = \"Colombiano\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        df_colombiano=pd.DataFrame(noticias,columns=['title', 'description','description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_colombiano\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_colombia = fuentes_colombiano('https://www.elcolombiano.com/colombia')\n",
    "fuente_antioquia = fuentes_colombiano('https://www.elcolombiano.com/antioquia')\n",
    "\n",
    "\n",
    "if fuente_colombia is None and fuente_antioquia is None:\n",
    "    df_colombiano = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_colombiano = pd.concat([fuente_colombia, fuente_antioquia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-section",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vanguardia\n",
    "def fuentes_vanguardia (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        descripcion = sel.css('h1.headline.font-1.bold span::text, div.r01-c01 div.headline.font-1.bold span::text').extract()\n",
    "        categoria = sel.css(\"div.header-section a::text\").extract()[0]\n",
    "        url = sel.css(\"div.template-vg-02 a::attr(href), div.portlet-boundary.portlet-static-end.teaser-viewer-portlet.advanced-vg-54.itr-reloadonpager ::attr(href)\").extract()\n",
    "        url = pd.DataFrame(url)\n",
    "        url = url.drop(url[url[0]=='https://www.vanguardia.com/exclusivo-suscriptores'].index)\n",
    "        url = url.drop(url[url[0]=='/judicial'].index)\n",
    "        url = url.drop(url[url[0]=='/colombia'].index)\n",
    "        url = url.drop_duplicates(keep='last')\n",
    "        url = url.values.tolist()\n",
    "        url =  [item for lista in url for item in lista]\n",
    "        url_n = []\n",
    "        for i in url:\n",
    "            i = i\n",
    "            numero = i[-5:]\n",
    "            pagina = i[0:4]\n",
    "            if numero.isnumeric() or pagina =='http':\n",
    "                url_n.append(i)\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url_n:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.vanguardia.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.vanguardia.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.paragraph p::text, div.paragraph span::text, div.paragraph b::text, div.paragraph a::text, div.paragraph strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] =  descripcion\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] =  dt_string\n",
    "        noticias['medium'] = \"Vanguardia\"\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_vanguardia = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_vanguardia\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "def fuentes_vanguardia_2 (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"div.r01-c01 div.headline.font-1.bold span::text\").extract()\n",
    "        descripcion = sel.css(\"div.r01-c01 p:first-child::text\").extract()\n",
    "        descripcion = pd.DataFrame(descripcion)\n",
    "        descripcion = descripcion.drop(descripcion[descripcion[0]==' '].index)\n",
    "        descripcion = descripcion.values.tolist()\n",
    "        descripcion = [item for lista in descripcion for item in lista]\n",
    "        l = len(descripcion)\n",
    "        categoria = sel.css(\"div.header-section a::text\").extract()[0]\n",
    "        fecha = sel.css(\"div.top div.date::text\").extract()\n",
    "        fecha = [i.split('\\n')[0].strip(' ').split(' ,')[0].replace(' ','-')for i in fecha]\n",
    "        fecha_v = []\n",
    "        for i in fecha:\n",
    "            if i[-5:] == 'horas' or i[0:4]=='hace':\n",
    "                fecha_v.append(dt_string)\n",
    "            else:\n",
    "                fecha_v.append(MesNumero(i))\n",
    "\n",
    "        url = sel.css(\"div.template-vg-28 a::attr(href)\").extract()\n",
    "        url = pd.DataFrame(url)\n",
    "        url = url.drop(url[url[0]=='https://www.vanguardia.com/exclusivo-suscriptores'].index)\n",
    "        url = url.drop_duplicates(keep='last')\n",
    "        url = url.values.tolist()\n",
    "        url =  [item for lista in url for item in lista]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.vanguardia.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.vanguardia.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"}, allow_redirects=False)\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.paragraph p::text, div.paragraph span::text, div.paragraph b::text, div.paragraph a::text, div.paragraph strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo)== len(descripcion):\n",
    "            noticias = {}\n",
    "            noticias['title'] =  titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha_v[:l]\n",
    "            noticias['medium'] = \"Vanguardia\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] =  descripcion\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] =  fecha_v[:l]\n",
    "            noticias['medium'] = \"Vanguardia\"\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        df_vanguardia=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_vanguardia\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_judicial = fuentes_vanguardia('https://www.vanguardia.com/judicial')\n",
    "fuente_colombia = fuentes_vanguardia('https://www.vanguardia.com/colombia')\n",
    "fuente_bucaramanga = fuentes_vanguardia_2('https://www.vanguardia.com/area-metropolitana/bucaramanga')\n",
    "fuente_floridablanca = fuentes_vanguardia_2('https://www.vanguardia.com/area-metropolitana/floridablanca')\n",
    "fuente_giron = fuentes_vanguardia_2('https://www.vanguardia.com/area-metropolitana/giron')\n",
    "fuente_piedecuesta = fuentes_vanguardia_2('https://www.vanguardia.com/area-metropolitana/piedecuesta')\n",
    "fuente_barrancabermeja = fuentes_vanguardia_2('https://www.vanguardia.com/santander/barrancabermeja')\n",
    "fuente_guanenta = fuentes_vanguardia_2('https://www.vanguardia.com/santander/guanenta')\n",
    "fuente_comunera = fuentes_vanguardia_2('https://www.vanguardia.com/santander/comunera')\n",
    "fuente_region = fuentes_vanguardia_2('https://www.vanguardia.com/santander/region')\n",
    "\n",
    "if fuente_judicial is None and fuente_colombia is None and fuente_bucaramanga is None and fuente_floridablanca is None and fuente_giron is None and fuente_piedecuesta is None and fuente_barrancabermeja is None and fuente_guanenta is None and fuente_comunera is None and fuente_region is None:\n",
    "    df_vanguardia = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_vanguardia = pd.concat([fuente_judicial, fuente_colombia, fuente_bucaramanga, fuente_floridablanca, fuente_giron, fuente_piedecuesta, fuente_barrancabermeja, fuente_guanenta, fuente_comunera, fuente_region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blu Radio\n",
    "def fuentes_blu (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h3.PromoC-title a::text, h3.PromoB-title div.PromoB-title-touch a::text\").extract()\n",
    "        titulo = limpieza_descripcion(titulo)\n",
    "        titulo =  [i.split('\\n  ')[0]for i in titulo]\n",
    "        categoria = sel.css(\"h1.SectionPage-pageHeading::text\").extract()[0]\n",
    "        descripcion = sel.css(\"h3.PromoC-title a::text, div.PromoB-description a::text\").extract()\n",
    "        descripcion = limpieza_descripcion(descripcion)\n",
    "        descripcion = [i.split('\\n  ')[0]for i in descripcion]\n",
    "        fecha = sel.css(\"div.PromoC-timestamp::attr(data-timestamp), div.PromoB-timestamp::attr(data-timestamp)\").extract()\n",
    "        fecha_b = [int(i)//1000 for i in fecha]\n",
    "        fecha_b = time_date(fecha_b)\n",
    "        url = sel.css(\"h3.PromoC-title a::attr(href), div.PromoB-description a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.bluradio.com/blu360{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.bluradio.com/blu360/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.Quote blockquote::text,div.RichTextArticleBody-body.RichTextBody p::text, div.RichTextArticleBody-body.RichTextBody b::text, div.RichTextArticleBody-body.RichTextBody strong::text, div.RichTextArticleBody-body.RichTextBody a::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r','') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] =  titulo\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] =  fecha_b\n",
    "        noticias['medium'] = \"Blu Radio\"\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_blu=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_blu\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_360 = fuentes_blu('https://www.bluradio.com/blu360')\n",
    "fuente_nacion = fuentes_blu('https://www.bluradio.com/nacion')\n",
    "fuente_bogota = fuentes_blu('https://www.bluradio.com/blu360/bogota')\n",
    "fuente_antioquia = fuentes_blu('https://www.bluradio.com/blu360/antioquia')\n",
    "fuente_pacifico = fuentes_blu('https://www.bluradio.com/blu360/pacifico')\n",
    "fuente_caribe = fuentes_blu('https://www.bluradio.com/blu360/caribe')\n",
    "fuente_santanderes = fuentes_blu('https://www.bluradio.com/blu360/santanderes')\n",
    "\n",
    "\n",
    "if fuente_360 is None and fuente_nacion is None and fuente_bogota is None and fuente_antioquia is None and fuente_pacifico is None and fuente_caribe is None and fuente_santanderes is None:\n",
    "    df_blu = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_blu = pd.concat([fuente_360, fuente_nacion, fuente_bogota, fuente_antioquia, fuente_pacifico ,fuente_caribe, fuente_santanderes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-workstation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# El Universal\n",
    "def fuentes_universal (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        categoria = sel.css(\"p.section-name ::text\").extract()[0]\n",
    "        descripcion = sel.css(\"div.general div.headline span.priority-content ::text\").extract()\n",
    "        l = len(descripcion)\n",
    "        fecha = sel.css(\"div.general div.fecha ::text\").extract()\n",
    "        fecha_universal = []\n",
    "        for i in fecha:\n",
    "            a = dt_string.split('/')    \n",
    "            i= i.split(' ')\n",
    "            if i[0] == '':\n",
    "                dia = i[2]\n",
    "                mes = i[1]\n",
    "            else:\n",
    "                dia = i[1]\n",
    "                mes = i[0]\n",
    "            anio = a[2]\n",
    "            fecha_universal.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "        url = sel.css(\"div.info a::attr(href)\").extract()[:l]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.eluniversal.com.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.eluniversal.com.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.paragraph p::text, div.paragraph strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = descripcion\n",
    "        noticias['description'] = descripcion\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = fecha_universal\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium'] = \"El Universal\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        df_el_universal = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_el_universal\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_cartagena = fuentes_universal(\"https://www.eluniversal.com.co/cartagena\")\n",
    "fuente_regional = fuentes_universal(\"https://www.eluniversal.com.co/regional\")\n",
    "fuente_colombia = fuentes_universal(\"https://www.eluniversal.com.co/colombia\")\n",
    "\n",
    "if fuente_cartagena is None and fuente_regional is None and fuente_colombia is None:\n",
    "    df_universal = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_universal = pd.concat([fuente_cartagena, fuente_regional, fuente_colombia])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-listing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hoy Diario del Magdalena\n",
    "def fuentes_magdalena (periodico_url):\n",
    "    try:  \n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo1 = sel.css(\"div.col-sm-12 h2.title a::text\").extract()\n",
    "        titulo1 = [i.replace('\\n', '') for i in titulo1]\n",
    "        titulo2 = sel.css(\"div.listing.listing-blog.listing-blog-5.clearfix h2.title a::text\").extract()\n",
    "        titulo2 = [i.replace('\\n', '') for i in titulo2]\n",
    "        titulo = titulo1 + titulo2\n",
    "        descripcion1 = sel.css(\"div.col-sm-12 h2.title a::text\").extract()\n",
    "        descripcion1 = [i.replace('\\n', '') for i in descripcion1]\n",
    "        descripcion2 = sel.css(\"div.listing.listing-blog.listing-blog-5.clearfix div.post-summary ::text\").extract()\n",
    "        descripcion2 = [i.replace('\\xa0', ' ') for i in descripcion2]\n",
    "        descripcion2 = limpieza_descripcion(descripcion2)\n",
    "        descripcion2 = [i.replace('\\n','') for i in descripcion2]\n",
    "        descripcion2 = pd.DataFrame(descripcion2)\n",
    "        descripcion2 = descripcion2.drop(descripcion2[descripcion2[0]==' '].index)\n",
    "        descripcion2 = descripcion2.drop(descripcion2[descripcion2[0]==''].index)\n",
    "        descripcion2 = descripcion2.drop(descripcion2[descripcion2[0]=='lee mas...'].index)\n",
    "        descripcion2 = descripcion2.values.tolist()\n",
    "        descripcion2 = [item for lista in descripcion2 for item in lista]\n",
    "        descripcion = descripcion1 + descripcion2\n",
    "        fecha1 = [dt_string] * len(titulo1)\n",
    "        fecha2 = sel.css(\"div.listing.listing-blog.listing-blog-5.clearfix span.time ::text\").extract()\n",
    "        fecha_magdalena2 = []\n",
    "        for i in fecha2:\n",
    "            i = i.split(' ')[0].split('/')\n",
    "            dia = i[0]      \n",
    "            mes = i[1]\n",
    "            anio = i[2]\n",
    "            fecha_magdalena2.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "        fecha = fecha1 + fecha_magdalena2\n",
    "        medio = \"Hoy Diario de Magdalena\"\n",
    "        categoria = sel.css(\"h1.page-heading ::text\").extract()[0]\n",
    "        url = sel.css(\"div.col-sm-12 h2.title a::attr(href), div.listing.listing-blog.listing-blog-5.clearfix h2.title a::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.hoydiariodelmagdalena.com.co/archivos{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.hoydiariodelmagdalena.com.co/archivos/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.entry-content.clearfix.single-post-content p::text, div.entry-content.clearfix.single-post-content strong::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        \n",
    "        if len(titulo) == len(descripcion): \n",
    "            descripcion = descripcion\n",
    "        else:\n",
    "            descripcion = titulo\n",
    "        for x in descripcion:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        if len(titulo) == len(descripcion):\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = descripcion\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = fecha\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium'] = medio\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "        else:\n",
    "            noticias = {}\n",
    "            noticias['title'] = titulo\n",
    "            noticias['description'] = titulo\n",
    "            noticias['description_long'] = variable_des\n",
    "            noticias['pubDate'] = fecha\n",
    "            noticias['category'] = categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium'] = medio\n",
    "            #--  Predict  \n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "        df_magdalena = pd.DataFrame(noticias, columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return df_magdalena\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "    \n",
    "\n",
    "fuente_magdalena = fuentes_magdalena(\"https://www.hoydiariodelmagdalena.com.co/archivos/category/magdalena\")\n",
    "fuente_locales = fuentes_magdalena(\"https://www.hoydiariodelmagdalena.com.co/archivos/category/locales\")\n",
    "fuente_regional = fuentes_magdalena(\"https://www.hoydiariodelmagdalena.com.co/archivos/category/regional\")\n",
    "fuente_judicial = fuentes_magdalena(\"https://www.hoydiariodelmagdalena.com.co/archivos/category/judicial\")\n",
    "fuente_judicialN = fuentes_magdalena(\"https://www.hoydiariodelmagdalena.com.co/archivos/category/judicial-nacional\")\n",
    "\n",
    "\n",
    "if fuente_magdalena is None and fuente_locales is None and fuente_regional is None and fuente_judicial is None and fuente_judicialN is None:\n",
    "    df_magdalena = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_magdalena = pd.concat([fuente_magdalena, fuente_locales, fuente_regional, fuente_judicial, fuente_judicialN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-brain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# La Opinión\n",
    "def fuentes_la_opinion (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        r_noticias.encoding = 'utf-8'\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"section.section span.field-content a::text\").extract()\n",
    "        url = sel.css(\"section.section span.field-content a::attr(href)\").extract()\n",
    "        categoria = sel.css(\"h1.title ::text\").extract()[0]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        fecha_opinion = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.laopinion.com.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.laopinion.com.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.clearfix.text-formatted.field.field--name-field-free-text.field--type-text-long.field--label-hidden.field__item p::text\").extract()\n",
    "                descripcion_larga = limpieza_descripcion(descripcion_larga)\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                fecha = sel.css(\"span.field.field--name-created.field--type-created.field--label-hidden::text\").extract()\n",
    "                for i in fecha:\n",
    "                    i = i.split(',')[-1]\n",
    "                    i = i.replace(' de ','-').replace(' ','')\n",
    "                    i = i.split('-')\n",
    "                    dia = i[0]\n",
    "                    mes = i[1]\n",
    "                    anio = i[2]\n",
    "                    fecha_opinion.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "                \n",
    "        if len(fecha_opinion) == len(variable_url):\n",
    "            fecha_opinion\n",
    "        else:\n",
    "            fecha_opinion = dt_string\n",
    "        \n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description'] = titulo\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = fecha_opinion\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium']=\"La Opinion\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        opinion=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return opinion\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_cucuta = fuentes_la_opinion('https://www.laopinion.com.co/cucuta')\n",
    "fuente_catatumbo = fuentes_la_opinion('https://www.laopinion.com.co/region/catatumbo')\n",
    "fuente_pamplona = fuentes_la_opinion('https://www.laopinion.com.co/region/pamplona')\n",
    "fuente_judicial = fuentes_la_opinion('https://www.laopinion.com.co/judicial')\n",
    "fuente_ocaña = fuentes_la_opinion('https://www.laopinion.com.co/region/ocana')\n",
    "\n",
    "if fuente_cucuta is None and fuente_catatumbo is None and fuente_pamplona is None and fuente_judicial is None and fuente_ocaña is None:\n",
    "    df_opinion = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_opinion = pd.concat([fuente_cucuta, fuente_catatumbo, fuente_ocaña, fuente_pamplona, fuente_judicial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-sugar",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# La Razón\n",
    "def fuentes_la_razon (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h2 a::text, div.meta.default-meta.side a::text, h4.bold.xt-post-title a::text\").extract()\n",
    "        url = sel.css(\"h2 a::attr(href), div.meta.default-meta.side a::attr(href), h4.bold.xt-post-title a::attr(href)\").extract()\n",
    "        categoria = periodico_url.split('/')[3]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        fecha_razon = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://larazon.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://larazon.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.post-body p::text, div.post-body strong::text, div.post-body br::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                fecha = sel.css(\"time.xt-post-date ::attr(datetime)\").extract()[0]\n",
    "                fecha = fecha.split('T')[0].split('-') \n",
    "                fecha_razon.append(fecha[2] + \"/\" +  fecha[1] + \"/\" + fecha[0])\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "                \n",
    "        if len(fecha_razon) == len(variable_url):\n",
    "            fecha_razon\n",
    "        else:\n",
    "            fecha_razon = dt_string\n",
    "            \n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description'] = titulo\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = fecha_razon\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium'] = \"La Razon\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        la_razon = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return la_razon\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "def fuentes_la_razon_1 (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h3.xt-archive-post-title.xt-post-title a::text\").extract()\n",
    "        fecha = sel.css(\"time.xt-post-date ::text\").extract()\n",
    "        fecha_razon = []\n",
    "        for i in fecha:\n",
    "            i = i.replace(',','')\n",
    "            i = i.split(' - ')[0].split(' ')\n",
    "            dia = i[0]      \n",
    "            mes = i[1]\n",
    "            anio = i[2]\n",
    "            fecha_razon.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "        url = sel.css(\"h3.xt-archive-post-title.xt-post-title a::attr(href)\").extract()\n",
    "        categoria = [i.split('/')[3] for i in url][0]\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://larazon.co{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://larazon.co/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"div.post-body p::text, div.post-body strong::text, div.post-body br::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == ' ':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description'] = titulo\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = fecha_razon\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium'] = \"La Razon\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        la_razon = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return la_razon\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_cordoba = fuentes_la_razon('https://larazon.co/seccion-cordoba/')\n",
    "fuente_monteria = fuentes_la_razon('https://larazon.co/seccion-monteria/')\n",
    "fuente_judicial = fuentes_la_razon('https://larazon.co/judiciales/')\n",
    "fuente_nacion = fuentes_la_razon_1('https://larazon.co/nacion/')\n",
    "\n",
    "\n",
    "if fuente_cordoba is None and fuente_monteria is None and fuente_judicial is None and fuente_nacion is None:\n",
    "    df_la_razon = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_la_razon = pd.concat([fuente_cordoba, fuente_monteria, fuente_judicial, fuente_nacion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-criterion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# El Espectador (Colombia)\n",
    "def fuentes_el_espectador (periodico_url):\n",
    "    try:\n",
    "        r_noticias=requests.get(periodico_url)\n",
    "        sel=Selector(r_noticias.text)\n",
    "\n",
    "        titulo = sel.css(\"h2.Card-Title.Title.Title a ::text\").extract()\n",
    "        fecha = sel.css(\"p.Card-Datetime.Datetime ::text\").extract()\n",
    "        categoria = periodico_url.split('/')[-2]\n",
    "        fecha_espectador = []\n",
    "        for i in fecha:\n",
    "            if i[-5:] == 'horas' or i[0:4]=='Hace':\n",
    "                fecha_espectador.append(dt_string)\n",
    "            else:\n",
    "                i = i.split('  - ')[0].split(' ')\n",
    "                dia = i[0]      \n",
    "                mes = i[1]\n",
    "                anio = i[2]\n",
    "                fecha_espectador.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "\n",
    "        url = sel.css(\"h2.Card-Title.Title.Title a ::attr(href)\").extract()\n",
    "        variable_url = []\n",
    "        variable_des = []\n",
    "        for item in url:\n",
    "            if item[0:4]=='http':\n",
    "                variable = item\n",
    "                variable_url.append(item)\n",
    "                r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                r_noticias.encoding = 'utf-8'\n",
    "                sel=Selector(r_noticias.text)  \n",
    "            else:\n",
    "                if item[:1] =='/':\n",
    "                    variable = 'https://www.elespectador.com{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "                else:\n",
    "                    variable = 'https://www.elespectador.com/{urls}'.format(urls=item)\n",
    "                    variable_url.append(variable)\n",
    "                    r_noticias=requests.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)\n",
    "            if r_noticias.status_code==200:\n",
    "                descripcion_larga = sel.css(\"p.font--secondary::text,div.Hook.ArticleHeader-Hook.Hook_main div::text,div.Hook.ArticleHeader-Hook.Hook_main::text\").extract()\n",
    "                descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = [i.replace('\\r',' ') for i in descripcion_larga]\n",
    "                descripcion_larga = ' '.join(descripcion_larga)\n",
    "                if descripcion_larga == '':\n",
    "                    valor = 'Sin descripcion en la noticia'\n",
    "                    variable_des.append(valor)\n",
    "                else:\n",
    "                    variable_des.append(descripcion_larga)\n",
    "            else:\n",
    "                variable_des.append('Error, pagina no encontrada')\n",
    "        catego = []\n",
    "        tipo = []\n",
    "        for x in titulo:\n",
    "            vect_text1 = lemant(x)\n",
    "            predi_categ = predCateg.predict([vect_text1])[0]\n",
    "            predi_type = predType.predict([vect_text1])[0]\n",
    "            catego.append(predi_categ)\n",
    "            tipo.append(predi_type)\n",
    "\n",
    "        noticias = {}\n",
    "        noticias['title'] = titulo\n",
    "        noticias['description'] = titulo\n",
    "        noticias['description_long'] = variable_des\n",
    "        noticias['pubDate'] = fecha_espectador\n",
    "        noticias['category'] = categoria\n",
    "        noticias['link'] = variable_url\n",
    "        noticias['medium'] = \"El Espectador\"\n",
    "        #--  Predict  \n",
    "        noticias['predi_type'] = tipo\n",
    "        noticias['predi_categ'] = catego\n",
    "\n",
    "        el_espectador = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "        return el_espectador\n",
    "    \n",
    "    except ValueError:\n",
    "        error_page.append(periodico_url)\n",
    "\n",
    "fuente_medellin = fuentes_el_espectador('https://www.elespectador.com/colombia/medellin/')\n",
    "fuente_cali = fuentes_el_espectador('https://www.elespectador.com/colombia/cali/')\n",
    "fuente_barranquilla = fuentes_el_espectador('https://www.elespectador.com/colombia/barranquilla/')\n",
    "fuente_cartagena = fuentes_el_espectador('https://www.elespectador.com/colombia/cartagena/')\n",
    "fuente_cundinamarca = fuentes_el_espectador('https://www.elespectador.com/colombia/cundinamarca/')\n",
    "\n",
    "if fuente_medellin is None and fuente_cali is None and fuente_barranquilla is None and fuente_cartagena is None and fuente_cundinamarca is None:\n",
    "    df_el_espectador = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_el_espectador = pd.concat([fuente_medellin, fuente_cali, fuente_barranquilla,fuente_cartagena, fuente_cundinamarca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Informador\n",
    "cs = cloudscraper.create_scraper(browser={'browser': 'firefox','platform': 'windows','mobile': False}, \n",
    "                                 delay=5, interpreter='nodejs')\n",
    "\n",
    "def fuentes_el_informador (periodico_url):\n",
    "    try:\n",
    "       \n",
    "        r_noticias = cs.get(periodico_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\n",
    "        sel=Selector(r_noticias.text)\n",
    "        soup = BeautifulSoup(r_noticias.content, 'html.parser')\n",
    "        if r_noticias.status_code==200:\n",
    "            titulo=soup.findAll('h2', class_='article-title')\n",
    "            titulo=[i.get_text().strip('\\n\\t') for i in titulo]\n",
    "            descripcion = soup.findAll('section', class_='article-intro')\n",
    "            descripcion=[i.get_text().replace('\\t','') for i in descripcion]\n",
    "            descripcion=[i.replace('\\n','') for i in descripcion]\n",
    "            descripcion=[i.strip('\\xa0') for i in descripcion]\n",
    "            categoria=soup.findAll('small', class_='subheading-category')\n",
    "            categoria=[i.get_text().strip() for i in categoria][0]\n",
    "            fecha =  sel.css(\"dd.published.hasTooltip meta\").extract() \n",
    "            fecha = [i.split() for i in fecha]\n",
    "            fecha = pd.DataFrame(fecha)\n",
    "            fecha = fecha.drop(fecha[fecha[1]!='itemprop=\"datePublished\"'].index)\n",
    "            fecha = fecha[2]\n",
    "            fecha = [i.split('=')[-1][1:-2].split('T')[0] for i in fecha]\n",
    "            fecha_informador = []\n",
    "            for i in fecha:\n",
    "                i = i.split('-') \n",
    "                dia = i[2]\n",
    "                mes = i[1]\n",
    "                anio = i[0]\n",
    "                fecha_informador.append(dia + \"/\" +  mes + \"/\" + anio)\n",
    "            if len(fecha_informador) == len(titulo):\n",
    "                fecha_informador = fecha_informador\n",
    "            else:\n",
    "                fecha_informador = dt_string\n",
    "            url =  sel.css(\"h2.article-title a::attr(href)\").extract() \n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    r_noticias=cs.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                    r_noticias.encoding = 'utf-8'\n",
    "                    sel=Selector(r_noticias.text)  \n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://www.elinformador.com.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=cs.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                    else:\n",
    "                        variable = 'https://www.elinformador.com.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        r_noticias=cs.get(variable, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "                        r_noticias.encoding = 'utf-8'\n",
    "                        sel=Selector(r_noticias.text)\n",
    "                if r_noticias.status_code==200:\n",
    "                    descripcion_larga = sel.css(\"section.article-content p::text, section.article-content br::text, section.article-content::text\").extract()\n",
    "                    descripcion_larga = [i.replace('\\r', '') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t', '') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\xa0', '') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n', '') for i in descripcion_larga]\n",
    "                    descripcion_larga =  ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == ' ':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            if len(titulo)== len(descripcion):\n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description']= descripcion\n",
    "                noticias['description_long']= variable_des\n",
    "                noticias['category']= categoria\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['pubDate'] = fecha_informador\n",
    "                noticias['medium']=\"El Informador\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego   \n",
    "            else:\n",
    "                noticias = {}\n",
    "                noticias = {}\n",
    "                noticias['title'] = titulo\n",
    "                noticias['description']= titulo\n",
    "                noticias['description_long']= variable_des\n",
    "                noticias['category']= categoria\n",
    "                noticias['link'] = variable_url\n",
    "                noticias['pubDate'] = fecha_informador\n",
    "                noticias['medium']=\"El Informador\"\n",
    "                #--  Predict\n",
    "                noticias['predi_type'] = tipo\n",
    "                noticias['predi_categ'] = catego \n",
    "\n",
    "            df_el_informador=pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_el_informador\n",
    "        \n",
    "        else:\n",
    "            print(periodico_url)\n",
    "\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        print (periodico_url) \n",
    "\n",
    "fuente_bolivar = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/181-bolivar')\n",
    "fuente_distrito = fuentes_el_informador('https://www.elinformador.com.co/index.php/el-magdalena/81-distrito')\n",
    "fuente_cienaga = fuentes_el_informador('https://www.elinformador.com.co/index.php/el-magdalena/82-cienaga')\n",
    "fuente_departamento = fuentes_el_informador('https://www.elinformador.com.co/index.php/el-magdalena/83-departamento')\n",
    "fuente_zona = fuentes_el_informador('https://www.elinformador.com.co/index.php/el-magdalena/219-zona-critica')\n",
    "fuente_guajira = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/77-la-guajira')\n",
    "fuente_atlantico = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/180-atlantico')\n",
    "fuente_cesar = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/182-cesar')\n",
    "fuente_cordoba = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/183-cordoba')\n",
    "fuente_sucre = fuentes_el_informador('https://www.elinformador.com.co/index.php/region-caribe/184-sucre')\n",
    "fuente_judiciales = fuentes_el_informador('https://www.elinformador.com.co/index.php/judiciales/71-judiciales-local')\n",
    "fuente_judicialesN = fuentes_el_informador('https://www.elinformador.com.co/index.php/judiciales/72-judiciales-nacional')\n",
    "\n",
    "if fuente_bolivar is None and fuente_distrito is None and fuente_cienaga is None and fuente_departamento is None  and fuente_zona is None  and fuente_guajira is None  and fuente_atlantico is None  and fuente_cesar is None  and fuente_cordoba is None  and fuente_sucre is None  and fuente_judiciales is None  and fuente_judicialesN is None:\n",
    "    df_el_informador = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_el_informador = pd.concat([fuente_bolivar, fuente_distrito, fuente_cienaga , fuente_departamento, fuente_zona, fuente_guajira, fuente_atlantico, fuente_cesar, fuente_cordoba, fuente_sucre, fuente_judiciales, fuente_judicialesN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El meridiano\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='C:/Users/harold.patino/Documents/Periodicos/chromedriver.exe', options=options)\n",
    "def fuente_meridiano(periodico_url):\n",
    "    try:\n",
    "        driver.implicitly_wait(10)\n",
    "        driver.get(periodico_url)\n",
    "        status = [i.text for i in driver.find_elements_by_tag_name('span.pl-0.border__span')]\n",
    "        if len(status) != 0:              \n",
    "            titulo = [i.text for i in driver.find_elements_by_tag_name('h2.News-content-title')]\n",
    "            l = len(titulo)\n",
    "            descripcion = [i.text for i in driver.find_elements_by_tag_name('p.News-content-description')]\n",
    "            descripcion = [i for i in descripcion if i !='']\n",
    "            categoria = [i.text for i in driver.find_elements_by_tag_name('span.News-content-details-city')][:l]\n",
    "            fecha_meridiano =[]\n",
    "            \n",
    "            fecha = [i.text for i in driver.find_elements_by_tag_name('span.News-content-details-date')][:l]\n",
    "            for i in fecha:\n",
    "                i = i.split(' ')[0]\n",
    "                variable = i.split('-')\n",
    "                variable = variable[0][0:1]\n",
    "                if variable in ('0','1','2','3','4','5','6','7','8','9'):\n",
    "                    fecha_meridiano.append(dt_string)\n",
    "                else:\n",
    "                    a = dt_string.split('/')\n",
    "                    variable_2 = i.split('-')\n",
    "                    dia = variable_2[1]\n",
    "                    mes = variable_2[0]\n",
    "                    anio = a[2]\n",
    "                    fecha_meridiano.append(MesNumero(dia + \"-\" +  mes + \"-\" + anio))\n",
    "                    \n",
    "            url = [i.get_attribute('href') for i in driver.find_elements_by_tag_name('a.no-hover, a.col-12')]\n",
    "            variable_url = []\n",
    "            variable_des = []\n",
    "            for item in url:\n",
    "                if item[0:4]=='http':\n",
    "                    variable = item\n",
    "                    variable_url.append(item)\n",
    "                    driver.get(variable)\n",
    "                else:\n",
    "                    if item[:1] =='/':\n",
    "                        variable = 'https://elmeridiano.co{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        driver.get(variable)\n",
    "                    else:\n",
    "                        variable = 'https://elmeridiano.co/{urls}'.format(urls=item)\n",
    "                        variable_url.append(variable)\n",
    "                        driver.get(variable)\n",
    "                if len(titulo)!=0:\n",
    "                    descripcion_larga = [i.text for i in driver.find_elements_by_tag_name('div.News-content-description, div.mb-3')]\n",
    "                    descripcion_larga = [i.replace('\\xa0',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\n',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = [i.replace('\\t',' ') for i in descripcion_larga]\n",
    "                    descripcion_larga = ' '.join(descripcion_larga)\n",
    "                    if descripcion_larga == '':\n",
    "                        valor = 'Sin descripcion en la noticia'\n",
    "                        variable_des.append(valor)\n",
    "                    else:\n",
    "                        variable_des.append(descripcion_larga)\n",
    "                else:\n",
    "                    variable_des.append('Error, pagina no encontrada')\n",
    "            catego = []\n",
    "            tipo = []\n",
    "            for x in descripcion:\n",
    "                vect_text1 = lemant(x)\n",
    "                predi_categ = predCateg.predict([vect_text1])[0]\n",
    "                predi_type = predType.predict([vect_text1])[0] \n",
    "                catego.append(predi_categ)\n",
    "                tipo.append(predi_type)\n",
    "\n",
    "            noticias = {} \n",
    "            noticias['title'] = titulo\n",
    "            noticias['description']= descripcion\n",
    "            noticias['description_long']= variable_des\n",
    "            noticias['pubDate'] = fecha_meridiano\n",
    "            noticias['category']= categoria\n",
    "            noticias['link'] = variable_url\n",
    "            noticias['medium']=\"El Meridiano\"\n",
    "            #--  Predict\n",
    "            noticias['predi_type'] = tipo\n",
    "            noticias['predi_categ'] = catego\n",
    "\n",
    "            df_meridiano = pd.DataFrame(noticias,columns=['title', 'description', 'description_long', 'pubDate', 'link', 'category','medium','processDate','predi_type','predi_categ'])\n",
    "            return df_meridiano\n",
    "\n",
    "        else:\n",
    "            df_meridiano = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "    \n",
    "    except ValueError:\n",
    "        df_meridiano = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "\n",
    "\n",
    "fuente_sucre = fuente_meridiano('https://elmeridiano.co/sucre/judicial')\n",
    "fuente_cordoba = fuente_meridiano('https://elmeridiano.co/cordoba/judicial')\n",
    "\n",
    "\n",
    "if fuente_cordoba is None and fuente_sucre is None:\n",
    "    df_meridiano = pd.DataFrame({'title': ['-'], 'description':['-'], 'description_long': ['-'], 'pubDate': ['-'],'link': ['-'], 'category': ['-'],'medium': ['-'], 'processDate': ['-'],'predi_type': ['FUERA_RANGO'], 'predi_categ': ['FUERA_RANGO']})\n",
    "else:\n",
    "    df_meridiano = pd.concat([fuente_cordoba, fuente_sucre])\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-security",
   "metadata": {},
   "source": [
    "## Union de Periodicos - Un solo DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "periodicos_union = [df_meridiano, df_nuevo_liberal, df_latarde, df_opinion, df_noticias_pilon, \n",
    "                    df_el_informador, df_la_razon, df_magdalena, df_colombiano, df_universal, df_blu, \n",
    "                    df_vanguardia, df_caracol, df_rcn_radio, df_noticias_PrensaLibre, df_hsb_noticias, \n",
    "                    df_nuevodia, df_noticias_ElTiempo, df_elpais, df_diarionorte, df_la_nacion, \n",
    "                    df_noticias_huila, df_siglo, df_diario_del_sur, df_putumayo, df_cronica_quindio, \n",
    "                    df_extra_judicial, df_cinaruco, df_la_patria, df_el_heraldo, df_policia, df_unidadvictimas, \n",
    "                    df_fuerza, df_ejercito, df_cauca, df_armada, df_el_espectador]\n",
    "periodicos_union_df = pd.concat(periodicos_union)\n",
    "periodicos_union_df = periodicos_union_df.assign(processDate=dt_string)\n",
    "periodicos_union_df = periodicos_union_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_periodicos_limpios = periodicos_union_df[(periodicos_union_df['predi_type'] != 'FUERA_RANGO') | (periodicos_union_df['predi_categ'] != 'FUERA_RANGO')]\n",
    "df_periodicos_limpios = df_periodicos_limpios.drop_duplicates(['title'], keep='last')\n",
    "if dt_string[0] == '0':\n",
    "    df_periodicos_limpios = df_periodicos_limpios[(df_periodicos_limpios['pubDate'] == dt_string) | (df_periodicos_limpios['pubDate'] == ayer) | (df_periodicos_limpios['pubDate'] == dt_string_adicional) | (df_periodicos_limpios['pubDate'] == ayer_adicional)]\n",
    "else:\n",
    "    df_periodicos_limpios = df_periodicos_limpios[(df_periodicos_limpios['pubDate'] == dt_string) | (df_periodicos_limpios['pubDate'] == ayer)]\n",
    "df_periodicos_limpios = df_periodicos_limpios.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-advertising",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_periodicos_limpios.predi_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_periodicos_limpios.predi_categ.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(error_page) > 0:\n",
    "    f = open ('C:/Users/harold.patino/Documents/Periodicos/DeVops/Validar_periodico.txt','w')\n",
    "    f.write(str(error_page))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-custody",
   "metadata": {
    "scrolled": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "df_periodicos_limpios.to_csv('C:/Users/harold.patino/Documents/Periodicos/DeVops/Noticias_long.txt', sep='|', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
