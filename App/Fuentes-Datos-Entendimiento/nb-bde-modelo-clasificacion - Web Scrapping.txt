# Databricks notebook source
################################################  Instalación de librerias
%pip install spacy
%pip install bs4
%pip install requests
%pip install parsel

# COMMAND ----------

# MAGIC %sh
# MAGIC python -m spacy download es_core_news_lg
# MAGIC python -m spacy download es_core_news_sm
# MAGIC python -m spacy download es
# MAGIC python -m nltk.downloader all

# COMMAND ----------

################################################  Importación de librerías
import pandas as pd
import numpy as np


from unicodedata import normalize
import nltk
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model  import LinearRegression as LinReg
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.svm import LinearSVC
from spacy.lang.es.stop_words import STOP_WORDS
import matplotlib.pyplot as plt

import es_core_news_sm

from datetime import date
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from parsel import Selector

from azure.storage.blob import ContainerClient
from azure.storage.blob import BlobClient
from io import StringIO
import os
import re

nltk.download('stopwords')
nlp = es_core_news_sm.load()


# COMMAND ----------

################################################  CADENA DE CONEXION BLOB STORAGE 
cnn = "DefaultEndpointsProtocol=https;AccountName=stuvictimasbitacora;AccountKey=c1H0TrAzPCjXnI1kJwL7gXXVTTbmfJcHTh8ANaRiinsLreGs+aBz8kICbASM7bhl46Jj7zMGFD+0ukEj0yelhA==;EndpointSuffix=core.windows.net"
container_client = ContainerClient.from_connection_string(conn_str=cnn, container_name="bitacora-modelo")

# COMMAND ----------

def leer_blob(archivo):
  blob_client = BlobClient.from_connection_string(conn_str=cnn, container_name="bitacora-modelo", blob_name=archivo)
  download_stream  = blob_client.download_blob()
  download_text = download_stream.content_as_text(max_concurrency=1, encoding='latin-1')
  df = pd.read_csv(StringIO(download_text),sep=';')
  return df

TAG_RE = re.compile(r'<[^>]+>')
def remove_tags(text):
    text = text.lower()
    text = re.sub('<.*?>', '', text)
    text = re.sub(':.*?:', '', text)
    text = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", normalize( "NFD", text), 0, re.I)
    text = text.replace(',', '')
    text = text.replace(';', '')
    return TAG_RE.sub('', text)

# COMMAND ----------

def deletestopwords(text):
    word_tokens = nltk.word_tokenize(text)
    stop_words_sp = STOP_WORDS # WITH SPACY
    word_tokens_clean = [each for each in word_tokens if each.lower() not in stop_words_sp and len(each.lower()) > 2]
    return word_tokens_clean

def lemmatizer(text):  
  doc = nlp(text)
  return ' '.join([word.lemma_ for word in doc])

def formatingText(text):
    text = text.lower()
    text = re.sub('<.*?>', '', text)
    text = re.sub(':.*?:', '', text)
    text = re.sub(r"([^n\u0300-\u036f]|n(?!\u0303(?![\u0300-\u036f])))[\u0300-\u036f]+", r"\1", normalize( "NFD", text), 0, re.I)
    text = normalize( 'NFC', text)
    text = re.sub('[^a-z ]', '', text)
    return text

def lemant(text1):
    x=text1.lower()
    stopwords= deletestopwords(x)
    stopwords=str(stopwords)
    clean = formatingText(stopwords)
    final = lemmatizer(clean)
    return final

# COMMAND ----------

################################################ MODELOS
def model_cat():
    UARIV_DF = leer_blob("NoticiasDataV01.csv")
        
    UARIV_DF['REVIEW'] = UARIV_DF['REVIEW'].astype(str)
    UARIV_DF["REVIEW_clean"] = UARIV_DF["REVIEW"].apply(lambda x: formatingText(x))
    UARIV_DF["REVIEW_stopword"] = UARIV_DF["REVIEW_clean"].apply(lambda x:  " ".join(deletestopwords(x)))
    UARIV_DF['lemmatized'] = UARIV_DF["REVIEW_stopword"].apply(lambda x: lemmatizer(x)).apply(lambda x: formatingText(x))
    stop_words_sp = STOP_WORDS
    tipo_de_evento = UARIV_DF["TIP_EVEN"]
    cat_evento = UARIV_DF["CAT_EVENTO"]

    x_train, x_test, y_train, y_test = train_test_split(UARIV_DF['lemmatized'], cat_evento, test_size=0.20)
    # MODELO PARA LAS 4 CATEGORIAS PRINCIPALES
    pipeline = Pipeline([('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words=stop_words_sp, sublinear_tf=True)),
                         ('chi',  SelectKBest(chi2, k=10000)),
                         ('clf', LinearSVC(C=1.0, penalty='l1', max_iter=3000, dual=False))])
    model = pipeline.fit(x_train, y_train)
    vectorizer = model.named_steps['vect']
    chi = model.named_steps['chi']
    clf = model.named_steps['clf']
    # Con ngram_range=(1,2) se le está diciendo al modelo que considere un conjunto de 2 palabras.
    return model

def model_type():

    UARIV_DF = leer_blob("NoticiasDataV01.csv")

    UARIV_DF['REVIEW'] = UARIV_DF['REVIEW'].astype(str)
    UARIV_DF["REVIEW_clean"] = UARIV_DF["REVIEW"].apply(lambda x: formatingText(x))
    UARIV_DF["REVIEW_stopword"] = UARIV_DF["REVIEW_clean"].apply(lambda x:  " ".join(deletestopwords(x)))
    UARIV_DF['lemmatized'] = UARIV_DF["REVIEW_stopword"].apply(lambda x: lemmatizer(x)).apply(lambda x: formatingText(x))
    stop_words_sp = STOP_WORDS
    tipo_de_evento = UARIV_DF["TIP_EVEN"]
    cat_evento = UARIV_DF["CAT_EVENTO"]
    
    x_train, x_test, y_train, y_test = train_test_split(UARIV_DF['lemmatized'], tipo_de_evento, test_size=0.20)
    # MODELO PARA LAS 14 TIPOS
    pipeline = Pipeline([('vect', TfidfVectorizer(ngram_range=(1, 2), stop_words=stop_words_sp, sublinear_tf=True)),
                         ('chi',  SelectKBest(chi2, k=10000)),
                         ('clf', LinearSVC(C=1.0, penalty='l1', max_iter=3000, dual=False))])
    model = pipeline.fit(x_train, y_train)
    vectorizer = model.named_steps['vect']
    chi = model.named_steps['chi']
    clf = model.named_steps['clf']
    # Con ngram_range=(1,2) se le está diciendo al modelo que considere un conjunto de 2 palabras.
    return model

# COMMAND ----------

predCateg = model_cat()
predType = model_type()

i=2101

################################################ El Tiempo
r = requests.get("https://www.eltiempo.com/rss/justicia_conflicto-y-narcotrafico.xml")
soup = BeautifulSoup(r.content, features ="html")
items = soup.findAll('item')
news_items = []
for item in items:
    news_text = remove_tags(item.description.text)
    news_item = {}
    news_item['id_not'] = i
    news_item['title'] = item.title.text
    news_item['description']= news_text
    news_item['pubDate'] = item.pubDate
    news_item['link'] = item.guid.text
    news_item['category']=item.category.text
    news_item['medium']="El Tiempo"
    news_item['processDate']=datetime.now()
    #--  Predict
    vect_text1 = lemant(news_text)
    news_item['predi_categ'] = predCateg.predict([vect_text1])[0]
    news_item['predi_type'] = predType.predict([vect_text1])[0]    
    news_items.append(news_item)
    i=i+1
df_rss_eltiempo = pd.DataFrame(news_items, columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium','processDate','predi_categ','predi_type'])


# COMMAND ----------

################################################ EL Nuevo Siglo (Bogotá)
r=requests.get("https://www.elnuevosiglo.com.co/rss.xml")
soup = BeautifulSoup(r.content, features ="xml")
items = soup.findAll('item')

news_items = []
for item in items:
    news_text = remove_tags(item.description.text)
    news_text = news_text.replace('\n', '')
    news_item = {}
    news_item['id_not'] = i
    news_item['title'] = item.title.text
    news_item['description']= news_text
    news_item['pubDate'] = item.pubDate.text
    news_item['link'] = item.link.text
    news_item['category']=""
    news_item['medium']="El Nuevo Siglo"
    news_item['processDate']=datetime.now()
    #--  Predict
    vect_text1 = lemant(news_text)
    news_item['predi_categ'] = predCateg.predict([vect_text1])[0]
    news_item['predi_type'] = predType.predict([vect_text1])[0]    
    news_items.append(news_item)
    i=i+1
df_rss_elsiglo = pd.DataFrame(news_items, columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium','processDate','predi_categ','predi_type'])

# COMMAND ----------

################################################ El Heraldo
r=requests.get('https://www.elheraldo.co/judicial')
soup = BeautifulSoup(r.text, 'html.parser')
items = soup.find_all('article', attrs={'class':'item zoom'})
news_items = []
for item in items:
    nota_txt = item.find('div', attrs={'class':'text'})
    link_n = nota_txt.find('h1').find('a')['href']
    link_n = 'https://www.elheraldo.co'+link_n
    r = requests.get(link_n)
    soup2 = BeautifulSoup(r.text, 'html.parser')
    titulo_nota = soup2.find_all('title')
    title_n = titulo_nota[0].text.strip()
    pubDate_n = soup2.find("meta",  {"name":"cXenseParse:recs:publishtime"})
    pubDate_n = pubDate_n["content"] if pubDate_n else None
    desc_nota = soup2.find_all('div', attrs={'id':'body'})
    description_n = ''
    description_n = desc_nota[0].text.strip()
    
    news_text = remove_tags(description_n)
    news_text = news_text.replace('\n', '')
    title_n = remove_tags(title_n)
    title_n = title_n.replace('\n', '')
    
    news_item = {}
    news_item['id_not'] = i
    news_item['title'] = title_n
    
    t1 = [re.sub(r"[^a-zA-Z0-9]+", ' ', k) for k in news_text.split("\n")]
    
    news_item['description']= t1
    news_item['pubDate'] = pubDate_n
    news_item['link'] = link_n
    news_item['category']="Judicial"
    news_item['medium']="El Heraldo"
    news_item['processDate']=datetime.now()
    #--  Predict
    vect_text1 = lemant(news_text)
    news_item['predi_categ'] = predCateg.predict([vect_text1])[0]
    news_item['predi_type'] = predType.predict([vect_text1])[0]    
    news_items.append(news_item)
    i=i+1
df_html_elheraldo = pd.DataFrame(news_items, columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium','processDate','predi_categ','predi_type'])

# COMMAND ----------

i=5000
################################################ El Tiempo - Bogotá

noticia_ET_bogota = "https://www.eltiempo.com/bogota"
r_noticias=requests.get(noticia_ET_bogota)
sel=Selector(r_noticias.text)

noticias = []
for noticia in r_noticias:
    
    noticias = {}
    noticias['title'] = sel.css("div.col2 h3.titulo a ::text, div.noticias-col-derecha-sec-bk h3.titulo a ::text").extract()
    noticias['description']= sel.css("div.lead ::attr(href)").extract()
    noticias['description'] = list(map(lambda d: d.replace('-', ' ')[8:-6].capitalize(), noticias['description']))
    noticias['pubDate'] = list(map(int,sel.css("span.actualizacion::attr(published-unix)").extract()))
    noticias['link'] = "https://www.eltiempo.com/"+ sel.css("h1.oculto ::text").extract()[0].strip('/')
    noticias['category']= "El Tiempo-" + sel.css("h1.oculto ::text").extract()[0].capitalize()
    noticias['medium']="El Tiempo"
    noticias['id_not'] = i
    noticias['processDate'] = datetime.now()
    #--  Predict
    str1 = ''.join(str(e) for e in noticias['description'])
    news_text = str1
    vect_text1 = lemant(news_text)
    
    noticias['predi_categ'] = predCateg.predict([vect_text1])[0]
    noticias['predi_type'] = predType.predict([vect_text1])[0]  
      
###Pasar de formato hora unix a hora comun
    for item in noticias['pubDate']:
        item = datetime.fromtimestamp(item)
    noticias['pubDate'] = item
noticia_ET_bogota = "https://www.eltiempo.com/bogota"
r_noticias=requests.get(noticia_ET_bogota)
sel=Selector(r_noticias.text)


noticias = []
for noticia in r_noticias:
    
    noticias = {}
    noticias['title'] = sel.css("div.col2 h3.titulo a ::text, div.noticias-col-derecha-sec-bk h3.titulo a ::text").extract()
    noticias['description']= sel.css("div.lead ::attr(href)").extract()
    noticias['description'] = list(map(lambda d: d.replace('-', ' ')[8:-6].capitalize(), noticias['description']))
    noticias['pubDate'] = list(map(int,sel.css("span.actualizacion::attr(published-unix)").extract()))
    noticias['link'] = "https://www.eltiempo.com/"+ sel.css("h1.oculto ::text").extract()[0].strip('/')
    noticias['category']= "El Tiempo-" + sel.css("h1.oculto ::text").extract()[0].capitalize()
    noticias['medium']="El Tiempo-Bogotá"
#------------- Nuevo codigo
    noticias['id_not'] = i
    noticias['processDate'] = datetime.now()
      
###Pasar de formato hora unix a hora comun
    for item in noticias['pubDate']:
        item = datetime.fromtimestamp(item)
    noticias['pubDate'] = item
    
df_bogota = pd.DataFrame(noticias,columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium'])
df_bogota['predi_categ'] = predCateg.predict(df_bogota['description'])[0]
df_bogota['predi_type'] = predType.predict(df_bogota['description'])[0]
df_bogota

# COMMAND ----------

i=6000
################################################ El Tiempo - Bogotá
noticia_ET_medellin = "https://www.eltiempo.com/colombia/medellin"
r_noticias=requests.get(noticia_ET_medellin)
sel=Selector(r_noticias.text)

noticias = []
for noticia in r_noticias:
    cad = sel.css("div.lead ::attr(href)").extract()
    cad = list(map(lambda d: d.replace('-', ' ')[19:-7].capitalize(), cad))
    noticias = {}
    noticias['id_not'] = i
    noticias['description']= cad
    l = len(noticias['description'])
    noticias['title'] = sel.css("h3>a::text").extract()[:l]
    noticias['category']= "El Tiempo-" + sel.css("h1.oculto ::text").extract()[0].capitalize()
    noticias['link'] = "https://www.eltiempo.com/colombia/"+ sel.css("h1.oculto ::text").extract()[0]
    noticias['pubDate'] = sel.css("span.actualizacion ::text").extract()
    noticias['medium']="El Tiempo-Medellín"
 
    #------------- Nuevo codigo
    noticias['id_not'] = i
    noticias['processDate'] = datetime.now()
    
df_medellin = pd.DataFrame(noticias,columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium'])
df_medellin['predi_categ'] = predCateg.predict(df_medellin['description'])[0]
df_medellin['predi_type'] = predType.predict(df_medellin['description'])[0]
df_medellin


# COMMAND ----------

################################################ Guarda el archivo
#df_final = pd.concat([df_rss_eltiempo,df_rss_elsiglo,df_html_elheraldo])
#df_final = pd.concat([df_rss_eltiempo,df_rss_elsiglo,df_html_elheraldo,df_medellin])
df_final = pd.concat([df_rss_eltiempo,df_rss_elsiglo,df_html_elheraldo,df_medellin,df_bogota])

nombre_archivo =  "BDE_NOTICIAS.csv"

datos = df_final.to_csv (index_label="idx", encoding = "UTF-8")
blob_client = container_client.upload_blob(name=nombre_archivo, data=datos,overwrite=True)

# COMMAND ----------



################################################ TEST
news_text = """
Tras un consejo extraordinario de seguridad, el alcalde de Sincelejo expidió un decreto que modifica la medida de toque de queda que regía en la ciudad de 12:00 p.m a 5:00 a.m y lo deja de 10:00 p.m a 5:00 a.m.del lunes 25 de enero.

De acuerdo con la administración municipal, la fuerza pública ejercerá un estricto control en diferentes puntos de la ciudad para evitar el incumplimiento a la norma y para prevenir la alteración del orden público.

El consejo de seguridad en el que se modificó el toque de queda fue decretado por el alcalde Gómez luego de los desórdenes protagonizados por un grupo de motociclistas durante una protesta frente a la Alcaldía a la que intentaron entrar por la fuerza.

El alcalde reveló que personal de inteligencia de la policía monitorea las redes sociales para detectar mensajes que inciten al desorden o alteración del orden público.

Igualmente, informó que Migración Colombia estará atenta a la presencia de extranjeros en las protestas y podría llegar a deportarlos.

La Infantería de Marina reforzará las labores de vigilancia en la ciudad para garantizar la tranquilidad de los ciudadanos, informó la Alcaldía.

“hacemos un llamado a la calma, estamos abiertos al diálogo con todos los gremios, tal como lo hemos hecho a lo largo de esta pandemia”, sostuvo el mandatario.

Indicó que su administración ha sostenido diálogos con el gremio de moto taxistas y comerciantes de la ciudad.

Tras los desórdenes durante la protesta de este viernes 22 de enero, fueron arrestadas 6 personas como presuntas generadoras de los disturbios, dos frente a la Alcaldía de Sincelejo y cuatro en otros sitios de la ciudad.
"""
vect_text1 = lemant(news_text)
predCateg = model_cat()
predType = model_type()

predi_categ = predCateg.predict([vect_text1])[0]
predi_type = predType.predict([vect_text1])[0]

print('Categoria - ',predi_categ)
print('Tipo - ',predi_type)

# COMMAND ----------

noticia_ET_bogota = "https://www.eltiempo.com/bogota"
r_noticias=requests.get(noticia_ET_bogota)
sel=Selector(r_noticias.text)


noticias = []
for noticia in r_noticias:
    
    noticias = {}
    noticias['title'] = sel.css("div.col2 h3.titulo a ::text, div.noticias-col-derecha-sec-bk h3.titulo a ::text").extract()
    noticias['description']= sel.css("div.lead ::attr(href)").extract()
    noticias['description'] = list(map(lambda d: d.replace('-', ' ')[8:-6].capitalize(), noticias['description']))
    noticias['pubDate'] = list(map(int,sel.css("span.actualizacion::attr(published-unix)").extract()))
    noticias['link'] = "https://www.eltiempo.com/"+ sel.css("h1.oculto ::text").extract()[0].strip('/')
    noticias['category']= "El Tiempo-" + sel.css("h1.oculto ::text").extract()[0].capitalize()
    noticias['medium']="El Tiempo"
    noticias['id_not'] = 1
    noticias['processDate'] = 1
    noticias['predi_categ'] = 1
    noticias['predi_type'] = 1
      
###Pasar de formato hora unix a hora comun
    for item in noticias['pubDate']:
        item = datetime.fromtimestamp(item)
    noticias['pubDate'] = item

df_bogota = pd.DataFrame(noticias,columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium','processDate','predi_categ','predi_type'])



# COMMAND ----------

noticia_ET_medellin = "https://www.eltiempo.com/colombia/medellin"
r_noticias=requests.get(noticia_ET_medellin)
sel=Selector(r_noticias.text)

noticias = []
for noticia in r_noticias:
    noticias = {}
    
    
    noticias['description']= sel.css("div.lead ::attr(href)").extract()
    noticias['description'] = list(map(lambda d: d.replace('-', ' ')[19:-7].capitalize(), noticias['description']))
    l = len(noticias['description'])
    noticias['title'] = sel.css("h3>a::text").extract()[:l]
    noticias['category']= "El Tiempo-" + sel.css("h1.oculto ::text").extract()[0].capitalize()
    noticias['link'] = "https://www.eltiempo.com/colombia/"+ sel.css("h1.oculto ::text").extract()[0]
    noticias['pubDate'] = sel.css("span.actualizacion ::text").extract()
    noticias['medium']="El Tiempo"
    noticias['id_not'] = 1
    noticias['processDate'] = 1
    noticias['predi_categ'] = 1
    noticias['predi_type'] = 1
    
df_medellin = pd.DataFrame(noticias,columns=['id_not','title', 'description', 'pubDate', 'link', 'category','medium','processDate','predi_categ','predi_type'])
df_medellin

# COMMAND ----------


