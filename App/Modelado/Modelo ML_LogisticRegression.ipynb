{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "modern-asset",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2022-04-04T17:10:20.869941+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.5\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : MSC v.1916 64 bit (AMD64)\n",
      "OS          : Windows\n",
      "Release     : 10\n",
      "Machine     : AMD64\n",
      "Processor   : Intel64 Family 6 Model 79 Stepping 1, GenuineIntel\n",
      "CPU cores   : 4\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-mexico",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harold.patino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model  import LinearRegression as LinReg\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import cloudscraper\n",
    "\n",
    "import es_core_news_sm\n",
    "nltk.download('stopwords')\n",
    "nlp = es_core_news_sm.load()\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from parsel import Selector\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "\n",
    "from io import StringIO\n",
    "import os\n",
    "import re\n",
    "import scrapy\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# Funciones adicionales\n",
    "error_page = []\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "delta = timedelta(days=1)\n",
    "ayer = now - delta\n",
    "ayer = ayer.strftime('%d/%m/%Y')\n",
    "\n",
    "# Funcion para agregar fechas sin inicio de cero\n",
    "dt_string_adicional = dt_string[1:]\n",
    "ayer_adicional = ayer[1:]\n",
    "\n",
    "\n",
    "def time_date(time):\n",
    "    fecha = []\n",
    "    for item in time:\n",
    "        fecha.append(datetime.fromtimestamp(int(item)).strftime('%d/%m/%Y'))\n",
    "    return(fecha)\n",
    "\n",
    "def MesNumero(string):\n",
    "    m = {'enero': \"01\",'january': \"01\",'febrero': \"02\",'february': \"02\",'marzo': \"03\",'march': \"03\",'abril': \"04\",'april': \"04\",'mayo': \"05\",'may': \"05\",'junio': \"06\",'june': \"06\",'julio': \"07\",'july': \"07\",'agosto': \"08\",'august': \"08\",'septiembre': \"09\",'september': \"09\",'octubre': \"10\",'october': \"10\",'noviembre': \"11\",'november': \"11\",'diciembre': \"12\",'december': \"12\"}\n",
    "    mcorto = {'ene': \"01\",'jan': \"01\",'feb': \"02\",'mar': \"03\",'abr': \"04\",'apr': \"04\",'may': \"05\",'jun': \"06\",'jul': \"07\",'ago': \"08\",'aug': \"08\",'sep': \"09\",'oct': \"10\",'nov': \"11\",'dic': \"12\",'dec': \"12\"}\n",
    "\n",
    "    fecha = string.split(\"-\")\n",
    "    dia =  fecha[0]\n",
    "    mes =  fecha[1]\n",
    "    anio = fecha[2]\n",
    "\n",
    "    try:\n",
    "        if len(mes) > 3:\n",
    "            mes_out = str(m[mes.lower()])\n",
    "        else:\n",
    "            mes_out = str(mcorto[mes.lower()])\n",
    "        \n",
    "        return(dia + \"/\" +  mes_out + \"/\" + anio)\n",
    "    except:\n",
    "        raise ValueError('No es un mes')\n",
    "\n",
    "def limpieza(fuentes):\n",
    "    agregar_dato=[]\n",
    "    for item in fuentes:\n",
    "        agregar_dato.append(item.replace('\\xa0', ''))\n",
    "    return(agregar_dato)\n",
    "\n",
    "def limpieza_descripcion(variable_corregir):\n",
    "    valor_limpio = list(map(lambda d: d.replace('\\r\\n', ''), variable_corregir))\n",
    "    valor_limpio = [i.strip('\\n') for i in valor_limpio]\n",
    "    valor_limpio = [i.strip('\\r') for i in valor_limpio]\n",
    "    valor_limpio = limpieza(valor_limpio)\n",
    "    \n",
    "    variable_almacenamiento = []\n",
    "    for x in valor_limpio:\n",
    "        x = remove_tags(x)\n",
    "        variable_almacenamiento.append(x)\n",
    "    return (variable_almacenamiento)\n",
    "\n",
    "def remove_caracter(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\t ', '', text) \n",
    "    return text\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub(':.*?:', '', text)\n",
    "    text = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", text), 0, re.I)\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace(';', '')\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def deletestopwords(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    stop_words_sp = STOP_WORDS # WITH SPACY\n",
    "    word_tokens_clean = [each for each in word_tokens if each.lower() not in stop_words_sp and len(each.lower()) > 2]\n",
    "    return word_tokens_clean\n",
    "\n",
    "def lemmatizer(text):  \n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc])\n",
    "\n",
    "def formatingText(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub(':.*?:', '', text)\n",
    "    text = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\", normalize( \"NFD\", text), 0, re.I)\n",
    "    text = normalize( 'NFC', text)\n",
    "    text = re.sub('[^a-z ]', '', text)\n",
    "    return text\n",
    "\n",
    "def lemant(text1):\n",
    "    x=text1.lower()\n",
    "    stopwords= deletestopwords(x)\n",
    "    stopwords=str(stopwords)\n",
    "    clean = formatingText(stopwords)\n",
    "    final = lemmatizer(clean)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-hartford",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "blessed-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"C:/Users/harold.patino/Documents/Periodicos/DeVops/stopwords-es.json\") as fname:\n",
    "    stopwords_es = json.load(fname)\n",
    "vectorizar_news = TfidfVectorizer(strip_accents='unicode', stop_words=stopwords_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "serious-disabled",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Machine Learning\n",
    "\n",
    "news = pd.read_csv(\"C:/Users/harold.patino/Documents/lectura_archivos/Data/Noticias_Data_Entry.csv\" , sep=';')\n",
    "# news = news[['REVIEW','TIP_EVEN','CAT_EVENTO']]\n",
    "\n",
    "# http://rasbt.github.io/mlxtend/\n",
    "class DenseTransformer(BaseEstimator):\n",
    "    def __init__(self, return_copy=True):\n",
    "        self.return_copy = return_copy\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if issparse(X):\n",
    "            return X.toarray()\n",
    "        elif self.return_copy:\n",
    "            return X.copy()\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X=X, y=y)\n",
    "\n",
    "# vectorizador =  TfidfVectorizer(strip_accents='unicode', stop_words=STOP_WORDS)\n",
    "# vectorizador.fit(news.REVIEW)\n",
    "\n",
    "# def model_type():\n",
    "    \n",
    "#     pipeline_logistica = make_pipeline (\n",
    "#     vectorizador,\n",
    "#     DenseTransformer(),\n",
    "#     LogisticRegression(C=5, verbose=1, random_state=0, penalty='l2')\n",
    "#     )\n",
    "#     modelo = pipeline_logistica.fit(X=news.REVIEW, y= news.TIP_EVEN)\n",
    "#     return modelo\n",
    "\n",
    "\n",
    "# def model_cat():\n",
    "   \n",
    "#     pipeline_logistica = make_pipeline (\n",
    "#     vectorizador,\n",
    "#     DenseTransformer(),\n",
    "#     LogisticRegression(C=5, verbose=1, random_state=0, penalty='l2')\n",
    "#     )\n",
    "#     modelo = pipeline_logistica.fit(X=news.REVIEW, y= news.CAT_EVENTO)\n",
    "#     return modelo\n",
    "\n",
    "# predCateg = model_cat()\n",
    "# predType = model_type()"
   ]
  },
  